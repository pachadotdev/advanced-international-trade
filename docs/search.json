[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced International Trade in R",
    "section": "",
    "text": "About\nI tried to replicate Feenstra’s results in R, because I had to learn a bit of Stata for the course ECO3300 (International Trade) taught at the University of Toronto. To organize my notes, I used Quarto to present my R outputs in a decently formatted HTML file.\nI have a printed copy of the first edition, so I am using the data and codes from the first edition.\nThe goal of these solutions is to provide a reference for those who come from Stata and want to learn R. I prioritized readability and simplicity over performance and elegance. There were parts of the code were it was challenging to stick to a literal code translation, and I had to use R idioms to make the code more readable.\nAs the exercises progress, I intentionally used more R idioms to make the code less repetitive and using R idioms, while keeping the code as readable as possible.\nThese notes have a public GitHub repository. The repository has a detailed track of changes, but not all the codes outside the Quarto documents therein are required as of June, 2024.\nWhen I started this project (Sept, 2023), the links from Prof. Feenstra’s website were broken, so I went to the Internet Archive Wayback Machine to find the linked site (The Center for International Data) from 2005-03-08 and I downloaded it with wget (i.e., I ran bash 00-download-wayback-backup.sh from the repository). This is not needed anymore because:\n\nI added the data files to the repository.\nProf. Feenstra’s website is now working, and he was really kind to fix the links when I asked him for permission to use the data and codes that I recovered.\n\nAll the datasets and Stata codes are intellectual property of Dr. Robert C. Feenstra. The R codes are of my authorship, but these are a translation of the Stata codes, so I released them under Creative Commons Zero v1.0 Universal.\nI appreciate the feedback that I received from Prof. Feenstra and Prof. Mingzhi “Jimmy” Xu.\nPlease do not hesistate to email me if this is useful, or if you have any questions or suggestions. My email is m.sepulveda@mail.utoronto.ca."
  },
  {
    "objectID": "chapter2.html#read-and-transform-the-data",
    "href": "chapter2.html#read-and-transform-the-data",
    "title": "Chapter 2. The Heckscher-Ohlin Model",
    "section": "Read and transform the data",
    "text": "Read and transform the data\n\nFeenstra’s code\n* This is to read the data into Stata *\n\nset mem 30m\n* insheet using c:\\Empirical_Exercise\\Chapter_2\\trefler.csv *\ninsheet using \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\hov_pub.csv\"\nrename v1 country\nrename v2 factor\nrename v3 AT\nrename v4 V\nrename v5 Y\nrename v6 B\nrename v7 YPC\nrename v8 POP\n\n* create country index *\nquietly summarize YPC\nlocal maxYPC=_result(6)\ngen ratio=YPC/`maxYPC'\nreplace ratio=ratio+0.0001 if country==\"Italy\"\n\nsort ratio\negen indexc=group(ratio)\n\n* create factor index *\nsort factor\negen indexf=group(factor)\n\n* include delta *\n\ngen delta=1\nreplace delta=0.03 if country==\"Bangladesh\"\nreplace delta=0.09 if country==\"Pakistan\"\nreplace delta=0.10 if country==\"Indonesia\"\nreplace delta=0.09 if country==\"Sri Lanka\"\nreplace delta=0.17 if country==\"Thailand\"\nreplace delta=0.16 if country==\"Colombia\"\nreplace delta=0.28 if country==\"Panama\"\nreplace delta=0.29 if country==\"Yugoslavia\"\nreplace delta=0.14 if country==\"Portugal\"\nreplace delta=0.11 if country==\"Uruguay\"\nreplace delta=0.45 if country==\"Greece\"\nreplace delta=0.55 if country==\"Ireland\"\nreplace delta=0.42 if country==\"Spain\"\nreplace delta=0.49 if country==\"Israel\"\nreplace delta=0.40 if country==\"Hong Kong\"\nreplace delta=0.38 if country==\"New Zealand\"\nreplace delta=0.60 if country==\"Austria\"\nreplace delta=0.48 if country==\"Singapore\"\nreplace delta=0.60 if country==\"Italy\"\nreplace delta=0.58 if country==\"UK\"\nreplace delta=0.70 if country==\"Japan\"\nreplace delta=0.65 if country==\"Belgium\"\nreplace delta=0.47 if country==\"Trinidad\"\nreplace delta=0.72 if country==\"Netherlands\"\nreplace delta=0.65 if country==\"Finland\"\nreplace delta=0.73 if country==\"Denmark\"\nreplace delta=0.78 if country==\"West Germany\"\nreplace delta=0.74 if country==\"France\"\nreplace delta=0.57 if country==\"Sweden\"\nreplace delta=0.69 if country==\"Norway\"\nreplace delta=0.79 if country==\"Switzerland\"\nreplace delta=0.55 if country==\"Canada\"\nreplace delta=1 if country==\"USA\"\n\ncompress\n\nlabel var country \"Name of the country\"\nlabel var factor \"Name of the factor\"\nlabel var AT \"Factor content of trade F=A*T\"\nlabel var V \"Endowment\"\nlabel var Y \"GDP, World Bank, y=p*Q\"\nlabel var B \"Trade balance, World Bank b=p*T\"\nlabel var YPC \"GDP per capita, PWT\"\nlabel var indexc \"Country indentifier\"\nlabel var indexf \"Factor Indentifier\"\n\n* save c:\\Empirical_Exercise\\Chapter_2\\trefler,replace *\nsave \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\trefler\", replace\n\nexit\n\n\nMy code\n\n# Packages ----\n\nlibrary(archive)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(knitr)\n\n# Extract ----\n\nfzip &lt;- \"first-edition/Chapter-2.zip\"\ndout &lt;- gsub(\"\\\\.zip$\", \"\", fzip)\n\nif (!dir.exists(dout)) {\n  archive_extract(fzip, dir = dout)\n}\n\n# Read ----\n\nfout &lt;- paste0(dout, \"/trefler.rds\")\nfout2 &lt;- paste0(dout, \"/trefler_desc.rds\")\n\nif (!file.exists(fout)) {\n  trefler &lt;- read_csv(\"first-edition/Chapter-2/hov_pub.csv\", col_names = F) %&gt;%\n    rename(\n      country = X1,\n      factor = X2,\n      at = X3,\n      v = X4,\n      y = X5,\n      b = X6,\n      ypc = X7,\n      pop = X8\n    )\n\n  # Transform ----\n\n  # see https://www.stata.com/manuals/rsummarize.pdf\n  # https://www.stata.com/manuals/degen.pdf\n  # https://www.stata.com/manuals/degen.pdf\n\n  # Create an auxiliary table for delta values\n  delta_values &lt;- tibble(\n    country = c(\n      \"Bangladesh\", \"Pakistan\", \"Indonesia\", \"Sri Lanka\", \"Thailand\",\n      \"Colombia\", \"Panama\", \"Yugoslavia\", \"Portugal\", \"Uruguay\", \"Greece\",\n      \"Ireland\", \"Spain\", \"Israel\", \"Hong Kong\", \"New Zealand\", \"Austria\",\n      \"Singapore\", \"Italy\", \"UK\", \"Japan\", \"Belgium\", \"Trinidad\", \"Netherlands\",\n      \"Finland\", \"Denmark\", \"West Germany\", \"France\", \"Sweden\", \"Norway\",\n      \"Switzerland\", \"Canada\", \"USA\"\n    ),\n    delta = c(\n      0.03, 0.09, 0.10, 0.09, 0.17, 0.16, 0.28, 0.29, 0.14, 0.11, 0.45,\n      0.55, 0.42, 0.49, 0.40, 0.38, 0.60, 0.48, 0.60, 0.58, 0.70, 0.65, 0.47,\n      0.72, 0.65, 0.73, 0.78, 0.74, 0.57, 0.69, 0.79, 0.55, 1\n    )\n  )\n\n  trefler &lt;- trefler %&gt;%\n    mutate(ypc_max = max(ypc)) %&gt;%\n    mutate(\n      ratio = case_when(\n        country != \"Italy\" ~ ypc / ypc_max,\n        country == \"Italy\" ~ (ypc / ypc_max) + 0.0001\n      )\n    ) %&gt;%\n    select(-ypc_max) %&gt;%\n    arrange(ratio) %&gt;%\n    group_by(ratio) %&gt;%\n    mutate(indexc = cur_group_id()) %&gt;%\n    group_by(factor) %&gt;%\n    mutate(indexf = cur_group_id()) %&gt;%\n    ungroup() %&gt;%\n    left_join(delta_values)\n\n  # Labels ----\n\n  # Create a separate table with the variables description\n\n  trefler_desc &lt;- tibble(\n    variable = c(\n      \"country\", \"factor\", \"at\", \"v\", \"y\", \"b\", \"ypc\", \"indexc\",\n      \"indexf\"\n    ),\n    description = c(\n      \"Name of the country\", \"Name of the factor\",\n      \"Factor content of trade F=A*T\", \"Endowment\", \"GDP, World Bank, y=p*Q\",\n      \"Trade balance, World Bank b=p*T\", \"GDP per capita, PWT\",\n      \"Country indentifier\", \"Factor Indentifier\"\n    )\n  )\n\n  # Save ----\n\n  saveRDS(trefler, fout)\n  saveRDS(trefler_desc, fout2)\n} else {\n  trefler &lt;- readRDS(fout)\n  trefler_desc &lt;- readRDS(fout2)\n}"
  },
  {
    "objectID": "chapter2.html#exercise-1",
    "href": "chapter2.html#exercise-1",
    "title": "Chapter 2. The Heckscher-Ohlin Model",
    "section": "Exercise 1",
    "text": "Exercise 1\nGiven identical technologies across countries, run the program sign_rank_1.do to conduct the sign test, rank test, and test for missing trade. Use the results in sign_rank_1.log to replicate columns (2) and (4) in Table 2.5.\n\nFeenstra’s code\n* This program is to conduct sign test, Rank test and Missing trade test *\n\ncapture log close\n* log using c:\\Empirical_Exercise\\Chapter_2\\sign_rank_1.log, replace *\nlog using \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\sign_rank_1.log\", replace\n\nset mem 30m\n\n* use C:\\Empirical_Exercise\\Chapter_2\\trefler, clear *\nuse \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\trefler\", clear\n\n* number of country in the dataset *\negen C=max(indexc)\negen F=max(indexf)\n\n* Calculate the world level of Yw, Bw and Vw *\negen Yww=sum(Y)\ngen Yw=Yww/F\negen Bww=sum(B)\ngen Bw=Bww/F\negen Vfw=sum(V), by(indexf)\n\n* Calculate country share Sc *\ngen Sc=(Y-B)/(Yw-Bw)\n\n* Calculate epsilon(fc) and sigma^2(f) according to eq.2 in Trefler (1995)*\ngen Efc=AT-(V-Sc*Vfw)\n\n* Construct the average epsilon for a given factor *\negen total=sum(Efc),by(indexf)\ngen ave=total/C\n\n* Construct sigma^2 and the weight *\n\negen tot=sum((Efc-ave)^2), by(indexf)\ngen sigma2f=tot/(C-1)\n\ncodebook sigma2f\ngen sigmaf=sqrt(sigma2f)\ngen weight=sigmaf*sqrt(Sc)\n\n* Using the weight, convert all the data *\n\ngen trAT=AT/(sigmaf*sqrt(Sc))\ngen trV=V/(sigmaf*sqrt(Sc))\ngen trY=Y/sqrt(Sc)\ngen trB=B/sqrt(Sc)\ngen trVfw=Vfw/sigmaf\n\ngen AThat=trV-Sc*trVfw\ngen AThat2=(V-Sc*Vfw)/weight\n\n* Correlation, should be .28 *\n\ncorr trAT AThat2\n\n*************\n* Sign Test *\n*************\n\nsort indexc\nby indexc: count if trAT*AThat2&gt;0\n\ncount if trAT*AThat2&gt;0\ndisplay _result(1)/_N\n\n*****************\n* Missing Trade *\n*****************\n\n* Checking for the missing trade, should be .032 *\n\nquietly summarize trAT\nlocal varAT=_result(4)\nquietly summarize AThat\nlocal varHat=_result(4)\nquietly summarize AThat2\nlocal varHat2=_result(4)\ndisplay `varAT'/`varHat'\ndisplay `varAT'/`varHat2'\n\n**************\n* Rank Tests *\n**************\n\nkeep country indexc indexf trAT AThat2\n\nsort indexc indexf\nreshape wide trAT AThat2, i(indexc) j(indexf)\n\nlocal i=1\nwhile `i'&lt;9{\n    local j=`i'+1\n    while `j'&lt;=9{\n        gen rank`i'`j'=((trAT`i'-trAT`j')*(AThat2`i'-AThat2`j')&gt;0)\n        local j=`j'+1\n    }\n    local i=`i'+1\n}\n\nkeep country indexc rank*\nreshape long rank, i(indexc) j(factor)\negen r1=sum(rank), by(indexc)\ngen r2=r1/36\ncollapse r2,by(indexc country)\nsum r2\nlist\n\nlog close\nexit\n\n\nMy code\n\n# Transform ----\n\ntrefler &lt;- readRDS(fout) %&gt;%\n  # Number of country in the dataset\n  mutate(\n    c = max(indexc),\n    f = max(indexf)\n  ) %&gt;%\n  # Calculate the world level of Yw, Bw and Vw\n  mutate(\n    yww = sum(y),\n    yw = yww / f,\n    bww = sum(b),\n    bw = bww / f\n  ) %&gt;%\n  group_by(indexf) %&gt;%\n  mutate(\n    vfw = sum(v)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Calculate country share Sc\n  mutate(\n    sc = (y - b) / (yw - bw)\n  ) %&gt;%\n  # Calculate epsilon(fc) and sigma^2(f) according to eq.2 in Trefler (1995)\n  mutate(\n    efc = at - (v - sc * vfw)\n  ) %&gt;%\n  # Construct the average epsilon for a given factor\n  group_by(indexf) %&gt;%\n  mutate(ave = sum(efc) / c) %&gt;%\n  # Construct sigma^2 and the weight\n  mutate(\n    sigma2f = sum((efc - ave)^2) / (c - 1),\n    sigmaf = sqrt(sigma2f),\n    weight = sigmaf * sqrt(sc)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Using the weight, convert all the data\n  mutate(\n    trat = at / (sigmaf * sqrt(sc)),\n    athat2 = (v - (sc * vfw)) / weight\n  ) %&gt;%\n  arrange(country)\n\n# Correlation, should be .28\ntrefler %&gt;%\n  select(trat, athat2) %&gt;%\n  cor()\n\n            trat    athat2\ntrat   1.0000000 0.2822883\nathat2 0.2822883 1.0000000\n\n# Sign Test ----\n\ntrefler %&gt;%\n  group_by(country, indexc) %&gt;%\n  summarize(\n    p = sum(trat * athat2 &gt; 0) / n()\n  )\n\n# A tibble: 33 × 3\n# Groups:   country [33]\n   country    indexc     p\n   &lt;chr&gt;       &lt;int&gt; &lt;dbl&gt;\n 1 Austria        17 0.556\n 2 Bangladesh      1 0.333\n 3 Belgium        22 0.667\n 4 Canada         32 0.556\n 5 Colombia        6 0.333\n 6 Denmark        26 0.444\n 7 Finland        25 0.333\n 8 France         28 0.333\n 9 Greece         11 0.111\n10 Hong Kong      15 0.667\n# ℹ 23 more rows\n\ntrefler %&gt;%\n  summarize(\n    p = sum(trat * athat2 &gt; 0) / n()\n  )\n\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.498\n\n# Missing Trade ----\n\n# Checking for the missing trade, should be .032\n\ntrefler %&gt;%\n  summarize(\n    varat = var(trat),\n    varhat2 = var(athat2)\n  ) %&gt;%\n  mutate(\n    varat_varhat2 = varat / varhat2\n  )\n\n# A tibble: 1 × 3\n  varat varhat2 varat_varhat2\n  &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1  1.61    50.3        0.0320\n\n# Rank Tests ----\n\ntrefler_wide &lt;- trefler %&gt;%\n  select(country, indexc, indexf, trat, athat2) %&gt;%\n  arrange(indexc, indexf) %&gt;%\n  pivot_wider(\n    names_from = indexf,\n    values_from = c(trat, athat2)\n  )\n\n# This would be too long\n# trefler_wide &lt;- trefler_wide %&gt;%\n#   mutate(\n#     rank12 = (trat_1 - trat_2) * (athat2_1 - athat2_2) &gt; 0,\n#     rank13 = (trat_1 - trat_3) * (athat2_1 - athat2_3) &gt; 0,\n#     ...\n#     rank89 = (trat_8 - trat_9) * (athat2_8 - athat2_9) &gt; 0,\n#   )\n\n# create all relevant trat_1 - trat_2, trat_1 - trat_3, etc.\n\nranks &lt;- expand_grid(\n  x = 1:8,\n  y = 1:9\n) %&gt;%\n  filter(x &lt; y)\n\n# The syntax here is based on internal R developer functions, but these\n# allow to create columns with minimal lines of code and avoids more complicated\n# bracket syntax\ntrefler_rank &lt;- map2_df(\n  pull(ranks, x),\n  pull(ranks, y),\n  function(x, y) {\n    trefler_wide %&gt;%\n      mutate(\n        name = paste0(\"rank\", x, y),\n        value = (!!sym(paste0(\"trat_\", x)) - !!sym(paste0(\"trat_\", y))) *\n          (!!sym(paste0(\"athat2_\", x)) - !!sym(paste0(\"athat2_\", y))) &gt; 0\n      ) %&gt;%\n      select(country, indexc, name, value)\n  }\n) %&gt;%\n  group_by(country, indexc) %&gt;%\n  summarise(r1 = sum(value)) %&gt;%\n  mutate(r2 = r1 / 36)\n\ntrefler_rank\n\n# A tibble: 33 × 4\n# Groups:   country [33]\n   country    indexc    r1     r2\n   &lt;chr&gt;       &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 Austria        17    19 0.528 \n 2 Bangladesh      1    27 0.75  \n 3 Belgium        22    22 0.611 \n 4 Canada         32    32 0.889 \n 5 Colombia        6    29 0.806 \n 6 Denmark        26    19 0.528 \n 7 Finland        25    17 0.472 \n 8 France         28     3 0.0833\n 9 Greece         11    17 0.472 \n10 Hong Kong      15    30 0.833 \n# ℹ 23 more rows\n\ntrefler_rank %&gt;%\n  pull(r2) %&gt;%\n  mean()\n\n[1] 0.6026936\n\n\n\n\nExtra step: formatting the table\n\ntrefler %&gt;%\n  group_by(country, indexc) %&gt;%\n  summarize(\n    p = sum(trat * athat2 &gt; 0) / n()\n  ) %&gt;%\n  arrange(indexc) %&gt;% # same order as in the book\n  select(country, sign_hov = p) %&gt;%\n  left_join(\n    trefler_rank %&gt;%\n      select(country, rank_hov = r2)\n  ) %&gt;%\n  bind_rows(\n    trefler %&gt;%\n      summarize(\n        p = sum(trat * athat2 &gt; 0) / n()\n      ) %&gt;%\n      mutate(\n        country = \"All countries\",\n        rank_hov = mean(pull(trefler_rank, r2))\n      ) %&gt;%\n      select(country, sign_hov = p, rank_hov)\n  ) %&gt;%\n  mutate_if(is.numeric, round, 2) %&gt;% # same no of decimals as in the book\n  kable()\n\n\n\n\ncountry\nsign_hov\nrank_hov\n\n\n\n\nBangladesh\n0.33\n0.75\n\n\nPakistan\n0.33\n0.72\n\n\nIndonesia\n0.22\n0.67\n\n\nSri Lanka\n0.22\n0.42\n\n\nThailand\n0.22\n0.69\n\n\nColombia\n0.33\n0.81\n\n\nPanama\n0.33\n0.56\n\n\nYugoslavia\n0.56\n0.44\n\n\nPortugal\n0.22\n0.53\n\n\nUruguay\n1.00\n0.72\n\n\nGreece\n0.11\n0.47\n\n\nIreland\n0.67\n0.53\n\n\nSpain\n0.22\n0.39\n\n\nIsrael\n0.67\n0.39\n\n\nHong Kong\n0.67\n0.83\n\n\nNew Zealand\n0.44\n0.53\n\n\nAustria\n0.56\n0.53\n\n\nSingapore\n0.56\n0.61\n\n\nItaly\n0.67\n0.78\n\n\nUK\n0.67\n0.58\n\n\nJapan\n0.78\n0.78\n\n\nBelgium\n0.67\n0.61\n\n\nTrinidad\n0.67\n0.50\n\n\nNetherlands\n0.44\n0.53\n\n\nFinland\n0.33\n0.47\n\n\nDenmark\n0.44\n0.53\n\n\nWest Germany\n0.56\n0.81\n\n\nFrance\n0.33\n0.08\n\n\nSweden\n0.44\n0.67\n\n\nNorway\n0.44\n0.61\n\n\nSwitzerland\n0.89\n0.56\n\n\nCanada\n0.56\n0.89\n\n\nUSA\n0.89\n0.92\n\n\nAll countries\n0.50\n0.60"
  },
  {
    "objectID": "chapter2.html#exercise-2",
    "href": "chapter2.html#exercise-2",
    "title": "Chapter 2. The Heckscher-Ohlin Model",
    "section": "Exercise 2",
    "text": "Exercise 2\nGiven uniform technological differences across countries, run the program sign_rank_2.do to redo the sign test, rank test, and missing trade. Use the results in sign_rank_2.log to replicate column (3) and (5), given column (6) in Table 2.5.\n\nFeenstra’s code\n* This program is to conduct sign test, Rank test and Missing trade test *\n* using delta *\n\ncapture log close\n* log using c:\\Empirical_Exercise\\Chapter_2\\sign_rank_2.log, replace *\nlog using \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\sign_rank_2.log\", replace\n\nset mem 30m\n\n* use c:\\Empirical_Exercise\\Chapter_2\\trefler, clear *\nuse \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\trefler\", clear\n\n* number of country in the dataset *\negen C=max(indexc)\negen F=max(indexf)\n\n* Calculate the world level of Yw, Bw and Vw *\negen Yww=sum(Y)\ngen Yw=Yww/F\negen Bww=sum(B)\ngen Bw=Bww/F\negen Vfw=sum(V), by(indexf)\negen Vw=sum(delta*V), by(indexf)\n\n* Calculate country share Sc *\ngen Sc=(Y-B)/(Yw-Bw)\n\n* Calculate epsilon(fc) and sigma^2(f) according to eq.2 in Trefler (1995)\n\ngen Efc=delta*AT-(delta*V-Sc*Vw)\n\n* Construct the average epsilon for a given factor *\negen total=sum(Efc),by(indexf)\ngen ave=total/C\n\n* Construct sigma^2 and the weight *\n\negen tot=sum((Efc-ave)^2), by(indexf)\ngen sigma2f=tot/(C-1)\n\ncodebook sigma2f\ngen sigmaf=sqrt(sigma2f)\ngen weight=sigmaf*sqrt(Sc)\n\n* Using the weight, convert all the data *\n\ngen trAT=delta*AT/weight\ngen AThat2=(delta*V-Sc*Vw)/weight\n\n* Correlation *\n\ncorr trAT AThat2\n\n*************\n* Sign Test *\n*************\n\nsort indexc\nby indexc: count if trAT*AThat2&gt;0\n\ncount if trAT*AThat2&gt;0\ndisplay _result(1)/_N\n\n*****************\n* Missing Trade *\n*****************\n\nquietly summarize trAT\nlocal varAT=_result(4)\nquietly summarize AThat\nlocal varHat=_result(4)\nquietly summarize AThat2\nlocal varHat2=_result(4)\ndisplay `varAT'/`varHat'\ndisplay `varAT'/`varHat2'\n\n*************\n* Rank Test *\n*************\n\nkeep country indexc indexf trAT AThat2\n\nsort indexc indexf\nreshape wide trAT AThat2, i(indexc) j(indexf)\n\nlocal i=1\nwhile `i'&lt;9{\n    local j=`i'+1\n    while `j'&lt;=9{\n        gen rank`i'`j'=((trAT`i'-trAT`j')*(AThat2`i'-AThat2`j')&gt;0)\n        local j=`j'+1\n    }\n    local i=`i'+1\n}\n\nkeep country indexc rank*\nreshape long rank, i(indexc) j(factor)\negen r1=sum(rank), by(indexc)\ngen r2=r1/36\ncollapse r2,by(indexc country)\nsum r2\nlist\n\nlog close\nexit\n\n\nMy code\n\n# Transform ----\n\ntrefler &lt;- readRDS(fout) %&gt;%\n  # Number of country in the dataset\n  mutate(\n    c = max(indexc),\n    f = max(indexf)\n  ) %&gt;%\n  # Calculate the world level of Yw, Bw and Vw\n  mutate(\n    yww = sum(y),\n    yw = yww / f,\n    bww = sum(b),\n    bw = bww / f\n  ) %&gt;%\n  group_by(indexf) %&gt;%\n  mutate(\n    vfw = sum(v),\n    vw = sum(delta * v)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Calculate country share Sc\n  mutate(\n    sc = (y - b) / (yw - bw)\n  ) %&gt;%\n  # Calculate epsilon(fc) and sigma^2(f) according to eq.2 in Trefler (1995)\n  mutate(\n    efc = delta * at - (delta * v - sc * vw)\n  ) %&gt;%\n  # Construct the average epsilon for a given factor\n  group_by(indexf) %&gt;%\n  mutate(ave = sum(efc) / c) %&gt;%\n  # Construct sigma^2 and the weight\n  mutate(\n    sigma2f = sum((efc - ave)^2) / (c - 1),\n    sigmaf = sqrt(sigma2f),\n    weight = sigmaf * sqrt(sc)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Using the weight, convert all the data\n  mutate(\n    trat = delta * at / weight,\n    athat2 = (delta * v - (sc * vw)) / weight\n  )\n\n# Correlation should be .41\ntrefler %&gt;%\n  select(trat, athat2) %&gt;%\n  cor()\n\n            trat    athat2\ntrat   1.0000000 0.4099617\nathat2 0.4099617 1.0000000\n\n# Sign Test ----\n\ntrefler %&gt;%\n  group_by(country, indexc) %&gt;%\n  summarize(\n    p = sum(trat * athat2 &gt; 0) / n()\n  )\n\n# A tibble: 33 × 3\n# Groups:   country [33]\n   country    indexc     p\n   &lt;chr&gt;       &lt;int&gt; &lt;dbl&gt;\n 1 Austria        17 0.667\n 2 Bangladesh      1 0.778\n 3 Belgium        22 0.778\n 4 Canada         32 0.222\n 5 Colombia        6 0.889\n 6 Denmark        26 0.444\n 7 Finland        25 0.444\n 8 France         28 0.333\n 9 Greece         11 0.556\n10 Hong Kong      15 0.889\n# ℹ 23 more rows\n\ntrefler %&gt;%\n  summarize(\n    p = sum(trat * athat2 &gt; 0) / n()\n  )\n\n# A tibble: 1 × 1\n      p\n  &lt;dbl&gt;\n1 0.620\n\n# Missing Trade ----\n\n# Checking for the missing trade, should be .07\n\ntrefler %&gt;%\n  summarize(\n    varat = var(trat),\n    varhat2 = var(athat2)\n  ) %&gt;%\n  mutate(\n    varat_varhat2 = varat / varhat2\n  )\n\n# A tibble: 1 × 3\n  varat varhat2 varat_varhat2\n  &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n1  1.34    19.0        0.0706\n\n# Rank Tests ----\n\ntrefler_wide &lt;- trefler %&gt;%\n  select(country, indexc, indexf, trat, athat2) %&gt;%\n  arrange(indexc, indexf) %&gt;%\n  pivot_wider(\n    names_from = indexf,\n    values_from = c(trat, athat2)\n  )\n\nranks &lt;- expand_grid(\n  x = 1:8,\n  y = 1:9\n) %&gt;%\n  filter(x &lt; y)\n\ntrefler_rank &lt;- map2_df(\n  pull(ranks, x),\n  pull(ranks, y),\n  function(x, y) {\n    trefler_wide %&gt;%\n      mutate(\n        name = paste0(\"rank\", x, y),\n        value = (!!sym(paste0(\"trat_\", x)) - !!sym(paste0(\"trat_\", y))) *\n          (!!sym(paste0(\"athat2_\", x)) - !!sym(paste0(\"athat2_\", y))) &gt; 0\n      ) %&gt;%\n      select(country, indexc, name, value)\n  }\n) %&gt;%\n  group_by(country, indexc) %&gt;%\n  summarise(r1 = sum(value)) %&gt;%\n  mutate(r2 = r1 / 36)\n\ntrefler_rank\n\n# A tibble: 33 × 4\n# Groups:   country [33]\n   country    indexc    r1    r2\n   &lt;chr&gt;       &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n 1 Austria        17    16 0.444\n 2 Bangladesh      1    28 0.778\n 3 Belgium        22    19 0.528\n 4 Canada         32    20 0.556\n 5 Colombia        6    31 0.861\n 6 Denmark        26    15 0.417\n 7 Finland        25    18 0.5  \n 8 France         28     7 0.194\n 9 Greece         11    27 0.75 \n10 Hong Kong      15    26 0.722\n# ℹ 23 more rows\n\ntrefler_rank %&gt;%\n  pull(r2) %&gt;%\n  mean()\n\n[1] 0.6153199\n\n\n\n\nExtra step: formatting the table\n\ntrefler %&gt;%\n  group_by(country, indexc) %&gt;%\n  summarize(\n    p = sum(trat * athat2 &gt; 0) / n()\n  ) %&gt;%\n  arrange(indexc) %&gt;% # same order as in the book\n  select(country, sign_hov = p) %&gt;%\n  left_join(\n    trefler_rank %&gt;%\n      select(country, rank_hov = r2)\n  ) %&gt;%\n  bind_rows(\n    trefler %&gt;%\n      summarize(\n        p = sum(trat * athat2 &gt; 0) / n()\n      ) %&gt;%\n      mutate(\n        country = \"All countries\",\n        rank_hov = mean(pull(trefler_rank, r2))\n      ) %&gt;%\n      select(country, sign_hov = p, rank_hov)\n  ) %&gt;%\n  mutate_if(is.numeric, round, 2) %&gt;% # same no of decimals as in the book\n  kable()\n\n\n\n\ncountry\nsign_hov\nrank_hov\n\n\n\n\nBangladesh\n0.78\n0.78\n\n\nPakistan\n0.67\n0.78\n\n\nIndonesia\n0.67\n0.67\n\n\nSri Lanka\n0.56\n0.67\n\n\nThailand\n0.67\n0.72\n\n\nColombia\n0.89\n0.86\n\n\nPanama\n0.78\n0.78\n\n\nYugoslavia\n0.67\n0.61\n\n\nPortugal\n0.78\n0.58\n\n\nUruguay\n0.11\n0.53\n\n\nGreece\n0.56\n0.75\n\n\nIreland\n0.44\n0.39\n\n\nSpain\n0.78\n0.69\n\n\nIsrael\n0.89\n0.69\n\n\nHong Kong\n0.89\n0.72\n\n\nNew Zealand\n0.22\n0.61\n\n\nAustria\n0.67\n0.44\n\n\nSingapore\n1.00\n0.61\n\n\nItaly\n0.33\n0.67\n\n\nUK\n0.78\n0.64\n\n\nJapan\n0.67\n0.78\n\n\nBelgium\n0.78\n0.53\n\n\nTrinidad\n1.00\n0.53\n\n\nNetherlands\n0.44\n0.47\n\n\nFinland\n0.44\n0.50\n\n\nDenmark\n0.44\n0.42\n\n\nWest Germany\n0.67\n0.78\n\n\nFrance\n0.33\n0.19\n\n\nSweden\n0.44\n0.36\n\n\nNorway\n0.44\n0.78\n\n\nSwitzerland\n0.89\n0.50\n\n\nCanada\n0.22\n0.56\n\n\nUSA\n0.56\n0.72\n\n\nAll countries\n0.62\n0.62\n\n\n\n\n\n\n\nNotes\nThe Stata code that I run returns the same values as R. However:\n\nAustria should have values 0.67 and 0.47. I got 0.67 and 0.44.\nFrance should have values 0.33 and 0.22. I got 0.33 and 0.19.\nSwitzerland should have values 0.89 and 0.47. I got 0.89 and 0.50."
  },
  {
    "objectID": "chapter2.html#exercise-3",
    "href": "chapter2.html#exercise-3",
    "title": "Chapter 2. The Heckscher-Ohlin Model",
    "section": "Exercise 3",
    "text": "Exercise 3\nAllowing all factors in each country to have different productivities, now run the program compute_pi.do to compute factor productivities \\(\\pi_k^i\\) as Trefler (1993). Note that there are 9 factors in the original data set, but these are now aggregated to just 4 factors, which are labor (endowment 1), capital (endowment 2), cropland (endowment 3) and pasture (endowment 4). Using the results in pi_.log or alternatively in the data files pi_1.dta, pi_2.dta, pi_3.dta, pi_4.dta to answer the following:\n\nWhich factor has the most negative productivities estimated?\nWhat is the correlation between the estimated labor productivity and the productivities of other factors? What is the correlation between each factor productivity and GDP per-capita (which you can find in the file trefler.dta)?\n\n\nFeenstra’s code\n* This program is to compute pai, the factor productivity *\n\ncapture log close\n* log using c:\\Empirical_Exercise\\Chapter_2\\pi.log,replace *\nlog using \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\pi.log\", replace\n\nset mem 30m\n\n* use c:\\Empirical_Exercise\\Chapter_2\\trefler, clear *\nuse \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\trefler\", clear\n\n* number of country in the dataset *\negen C=max(indexc)\negen F=max(indexf)\n\n* Calculate the world level of Yw, Bw and Vw *\negen Yww=sum(Y)\ngen Yw=Yww/F\negen Bww=sum(B)\ngen Bw=Bww/F\negen Vfw=sum(V), by(indexf)\n\n* Calculate country share Sc *\ngen Sc=(Y-B)/(Yw-Bw)\n\n* Calculate epsilon(fc) and sigma^2(f) according to eq.2 in Trefler (1995)\ngen Efc=AT-(V-Sc*Vfw)\n\n* Construct the average epsilon for a given factor *\negen total=sum(Efc),by(indexf)\ngen ave=total/C\n\n* Construct sigma^2 and the weight *\n\negen tot=sum((Efc-ave)^2), by(indexf)\ngen sigma2f=tot/(C-1)\n\ncodebook sigma2f\ngen sigmaf=sqrt(sigma2f)\ngen weight=sigmaf*sqrt(Sc)\n\n* Using the weight, convert all the data *\n\ngen trAT=AT/(sigmaf*sqrt(Sc))\ngen trV=V/(sigmaf*sqrt(Sc))\ngen trY=Y/sqrt(Sc)\ngen trB=B/sqrt(Sc)\ngen trVfw=Vfw/sigmaf\n\ngen AThat=trV-Sc*trVfw\ngen AThat2=(V-Sc*Vfw)/weight\n\n* Construct Aggregate Labor Endowment *\n\npreserve\nkeep if indexf==7 |indexf==8 | indexf==9\ngen en=2\nreplace en=3 if indexf==8\nreplace en=4 if indexf==9\nkeep country factor AT V en indexc Sc\n\n* save c:\\Empirical_Exercise\\Chapter_2\\indexf_189,replace *\nsave \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\indexf_189\", replace\n\nrestore\n\npreserve\ndrop if indexf==7 |indexf==8 | indexf==9\n\negen v_l=sum(V), by(country)\negen AT_l=sum(AT), by(country)\n\ndrop V AT factor\nrename v_l V\nrename AT_l AT\ngen en=1\ncollapse (mean)AT V en indexc Sc, by(country)\ngen str5 factor=\"Labor\"\n* save c:\\Empirical_Exercise\\Chapter_2\\indexf_L,replace *\nsave \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\indexf_L\", replace\n\nrestore\n\n* use c:\\Empirical_Exercise\\Chapter_2\\indexf_189,clear *\nuse \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\indexf_189\", clear\n\n* append using c:\\Empirical_Exercise\\Chapter_2\\indexf_L *\nappend using \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\indexf_L\"\nsort indexc en\n* save c:\\Empirical_Exercise\\Chapter_2\\pi,replace *\nsave \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\pi\", replace\n\n************************************\n* Compute Pi: factor productivity *\n************************************\n\n* use c:\\Empirical_Exercise\\Chapter_2\\pi, clear *\nuse \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\pi\", clear\n\ngen p_0=1\ngen p_1=1\n\nlocal i=1\nwhile `i'&lt;=4{\n    preserve\n    keep if en==`i'\n    local j=1\n    while `j'&lt;51{\n        replace p_0=p_1\n        gen Vp=p_0*V\n        egen Vpw=sum(Vp)\n        replace p_1=(AT+Sc*Vpw)/V\n        replace p_1=1 if country==\"USA\"\n        drop Vp Vpw\n        local j=`j'+1\n    }\n    keep en country indexc p_1\n    * save c:\\Empirical_Exercise\\Chapter_2\\pi_`i',replace *\n  save \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\pi_`i'\", replace\n    restore\n    local i=`i'+1\n}\n\nlocal i=1\nwhile `i'&lt;=4{\n    * use c:\\Empirical_Exercise\\Chapter_2\\pi_`i',clear *\n  use \"Z:\\home\\pacha\\github\\advanced-international-trade\\first-edition\\Chapter-2\\pi_`i'\", clear\n    sort en indexc\n    by en: list en indexc country p_1\n    local i=`i'+1\n}\n\nlog close\n\nexit\n\n\nMy code\n\ntrefler &lt;- readRDS(fout) %&gt;%\n  # Number of country in the dataset\n  mutate(\n    c = max(indexc),\n    f = max(indexf)\n  ) %&gt;%\n  # Calculate the world level of Yw, Bw and Vw\n  mutate(\n    yww = sum(y),\n    yw = yww / f,\n    bww = sum(b),\n    bw = bww / f\n  ) %&gt;%\n  group_by(indexf) %&gt;%\n  mutate(\n    vfw = sum(v),\n    vw = sum(delta * v)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Calculate country share Sc\n  mutate(\n    sc = (y - b) / (yw - bw)\n  ) %&gt;%\n  # Calculate epsilon(fc) and sigma^2(f) according to eq.2 in Trefler (1995)\n  mutate(\n    efc = delta * at - (delta * v - sc * vw)\n  ) %&gt;%\n  # Construct the average epsilon for a given factor\n  group_by(indexf) %&gt;%\n  mutate(ave = sum(efc) / c) %&gt;%\n  # Construct sigma^2 and the weight\n  mutate(\n    sigma2f = sum((efc - ave)^2) / (c - 1),\n    sigmaf = sqrt(sigma2f),\n    weight = sigmaf * sqrt(sc)\n  ) %&gt;%\n  ungroup() %&gt;%\n  # Using the weight, convert all the data\n  mutate(\n    trat = delta * at / weight,\n    athat2 = (delta * v - (sc * vw)) / weight\n  )\n\n# No need to save changes and restore\ntrefler2 &lt;- trefler %&gt;%\n  # Construct Aggregate Labor Endowment\n  filter(indexf %in% 7:9) %&gt;%\n  mutate(\n    en = case_when(\n      indexf == 7 ~ 2,\n      indexf == 8 ~ 3,\n      indexf == 9 ~ 4\n    )\n  ) %&gt;%\n  select(country, factor, indexf, at, v, en, indexc, sc)\n\ntrefler3 &lt;- trefler %&gt;%\n  filter(!(indexf %in% 7:9)) %&gt;%\n  group_by(country) %&gt;%\n  # No need to create v_l and at_l to then rename and then collapse\n  summarise(\n    v = sum(v),\n    at = sum(at),\n    sc = mean(sc),\n    indexc = mean(indexc)\n  ) %&gt;%\n  mutate(\n    en = 1,\n    factor = \"Labor\"\n  )\n\ntrefler3 &lt;- trefler2 %&gt;%\n  select(-indexf) %&gt;%\n  bind_rows(trefler3) %&gt;%\n  arrange(indexc, en)\n\n# Compute Pi: factor productivity\n# No need to save intermediate outputs, we proceed with iteration\ntrefler4 &lt;- map_df(\n  trefler3 %&gt;%\n    distinct(en) %&gt;%\n    pull(),\n  function(x) {\n    d &lt;- trefler3 %&gt;%\n      filter(en == x) %&gt;%\n      mutate(p0 = 1, p1 = 1)\n\n    iter &lt;- 50\n\n    for (i in seq_len(iter)) {\n      d &lt;- d %&gt;%\n        mutate(\n          p0 = p1,\n          vp = p0 * v,\n          vpw = sum(vp),\n          p1 = ifelse(country == \"USA\", 1, (at + sc * vpw) / v)\n        ) %&gt;%\n        select(-vp, -vpw)\n    }\n\n    # Tidy alternative to the for loop\n    # d &lt;- accumulate(seq_len(iter), .init = d, ~ .x %&gt;%\n    #   mutate(\n    #     p0 = p1,\n    #     p1 = ifelse(country == \"USA\", 1, (at + sc * sum(p1 * v)) / v)\n    #   )) %&gt;%\n    #   .[[iter + 1]]\n\n    d %&gt;%\n      select(country, indexc, en, p1)\n  }\n)\n\n\n\nExtra step: formatting the tables\n\ntrefler4 %&gt;%\n  arrange(country, en) %&gt;%\n  kable()\n\n\n\n\ncountry\nindexc\nen\np1\n\n\n\n\nAustria\n17\n1\n0.5694186\n\n\nAustria\n17\n2\n0.5421980\n\n\nAustria\n17\n3\n1.1596592\n\n\nAustria\n17\n4\n1.9753040\n\n\nBangladesh\n1\n1\n0.0165219\n\n\nBangladesh\n1\n2\n0.9119021\n\n\nBangladesh\n1\n3\n0.0230836\n\n\nBangladesh\n1\n4\n1.2561775\n\n\nBelgium\n22\n1\n0.7233988\n\n\nBelgium\n22\n2\n0.6457500\n\n\nBelgium\n22\n3\n0.4575561\n\n\nBelgium\n22\n4\n6.8868596\n\n\nCanada\n32\n1\n0.7799865\n\n\nCanada\n32\n2\n0.7359194\n\n\nCanada\n32\n3\n0.4738445\n\n\nCanada\n32\n4\n0.9541137\n\n\nColombia\n6\n1\n0.1940833\n\n\nColombia\n6\n2\n0.4856291\n\n\nColombia\n6\n3\n0.5963939\n\n\nColombia\n6\n4\n0.1271541\n\n\nDenmark\n26\n1\n0.7061784\n\n\nDenmark\n26\n2\n0.7035640\n\n\nDenmark\n26\n3\n1.7598990\n\n\nDenmark\n26\n4\n28.7283398\n\n\nFinland\n25\n1\n0.6969619\n\n\nFinland\n25\n2\n0.6453199\n\n\nFinland\n25\n3\n0.6789600\n\n\nFinland\n25\n4\n22.2966445\n\n\nFrance\n28\n1\n0.7356650\n\n\nFrance\n28\n2\n0.6150762\n\n\nFrance\n28\n3\n1.5589519\n\n\nFrance\n28\n4\n3.1663974\n\n\nGreece\n11\n1\n0.3140718\n\n\nGreece\n11\n2\n0.4401574\n\n\nGreece\n11\n3\n0.6072472\n\n\nGreece\n11\n4\n0.5059134\n\n\nHong Kong\n15\n1\n0.3781684\n\n\nHong Kong\n15\n2\n0.5150965\n\n\nHong Kong\n15\n3\n-229.0729738\n\n\nHong Kong\n15\n4\n-879.5376414\n\n\nIndonesia\n3\n1\n0.0432824\n\n\nIndonesia\n3\n2\n0.2506530\n\n\nIndonesia\n3\n3\n0.1641388\n\n\nIndonesia\n3\n4\n0.4576649\n\n\nIreland\n12\n1\n0.4166084\n\n\nIreland\n12\n2\n0.4783923\n\n\nIreland\n12\n3\n1.4803708\n\n\nIreland\n12\n4\n0.6129137\n\n\nIsrael\n14\n1\n0.6131932\n\n\nIsrael\n14\n2\n0.5049116\n\n\nIsrael\n14\n3\n2.5171947\n\n\nIsrael\n14\n4\n2.1567210\n\n\nItaly\n19\n1\n0.5907437\n\n\nItaly\n19\n2\n0.5122213\n\n\nItaly\n19\n3\n0.6419316\n\n\nItaly\n19\n4\n3.1452732\n\n\nJapan\n21\n1\n0.6049210\n\n\nJapan\n21\n2\n0.6502130\n\n\nJapan\n21\n3\n4.2923043\n\n\nJapan\n21\n4\n100.5984017\n\n\nNetherlands\n24\n1\n0.7933355\n\n\nNetherlands\n24\n2\n0.7513015\n\n\nNetherlands\n24\n3\n9.0914542\n\n\nNetherlands\n24\n4\n13.0978390\n\n\nNew Zealand\n16\n1\n0.5850298\n\n\nNew Zealand\n16\n2\n0.6197254\n\n\nNew Zealand\n16\n3\n7.5203295\n\n\nNew Zealand\n16\n4\n0.3673216\n\n\nNorway\n30\n1\n0.7816517\n\n\nNorway\n30\n2\n0.6199874\n\n\nNorway\n30\n3\n1.8129219\n\n\nNorway\n30\n4\n34.5985787\n\n\nPakistan\n2\n1\n0.0394124\n\n\nPakistan\n2\n2\n0.4867497\n\n\nPakistan\n2\n3\n0.0975686\n\n\nPakistan\n2\n4\n0.4731917\n\n\nPanama\n7\n1\n0.2279774\n\n\nPanama\n7\n2\n0.3224079\n\n\nPanama\n7\n3\n0.5834955\n\n\nPanama\n7\n4\n0.2973391\n\n\nPortugal\n9\n1\n0.1759864\n\n\nPortugal\n9\n2\n0.2971070\n\n\nPortugal\n9\n3\n-0.3833130\n\n\nPortugal\n9\n4\n1.6397757\n\n\nSingapore\n18\n1\n0.4873306\n\n\nSingapore\n18\n2\n0.3209652\n\n\nSingapore\n18\n3\n-25.4922493\n\n\nSingapore\n18\n4\n795.1006524\n\n\nSpain\n13\n1\n0.4172726\n\n\nSpain\n13\n2\n0.5194890\n\n\nSpain\n13\n3\n0.2365766\n\n\nSpain\n13\n4\n1.0176692\n\n\nSri Lanka\n4\n1\n0.0344433\n\n\nSri Lanka\n4\n2\n0.1259789\n\n\nSri Lanka\n4\n3\n0.2300838\n\n\nSri Lanka\n4\n4\n0.7949096\n\n\nSweden\n29\n1\n0.7270576\n\n\nSweden\n29\n2\n0.9445322\n\n\nSweden\n29\n3\n0.9073509\n\n\nSweden\n29\n4\n7.6468612\n\n\nSwitzerland\n31\n1\n0.9317447\n\n\nSwitzerland\n31\n2\n0.6079722\n\n\nSwitzerland\n31\n3\n3.7425295\n\n\nSwitzerland\n31\n4\n3.3350218\n\n\nThailand\n5\n1\n0.0489020\n\n\nThailand\n5\n2\n0.3929541\n\n\nThailand\n5\n3\n0.3002516\n\n\nThailand\n5\n4\n16.9392348\n\n\nTrinidad\n23\n1\n0.4483440\n\n\nTrinidad\n23\n2\n0.4193396\n\n\nTrinidad\n23\n3\n-0.9042060\n\n\nTrinidad\n23\n4\n4.6219780\n\n\nUK\n20\n1\n0.6332355\n\n\nUK\n20\n2\n0.8159872\n\n\nUK\n20\n3\n1.4765402\n\n\nUK\n20\n4\n2.2422499\n\n\nUSA\n33\n1\n1.0000000\n\n\nUSA\n33\n2\n1.0000000\n\n\nUSA\n33\n3\n1.0000000\n\n\nUSA\n33\n4\n1.0000000\n\n\nUruguay\n10\n1\n0.2029693\n\n\nUruguay\n10\n2\n0.4060566\n\n\nUruguay\n10\n3\n0.5709917\n\n\nUruguay\n10\n4\n0.0984165\n\n\nWest Germany\n27\n1\n0.7691833\n\n\nWest Germany\n27\n2\n0.6629313\n\n\nWest Germany\n27\n3\n1.0541740\n\n\nWest Germany\n27\n4\n6.8647780\n\n\nYugoslavia\n8\n1\n0.1997569\n\n\nYugoslavia\n8\n2\n0.2985477\n\n\nYugoslavia\n8\n3\n0.3189656\n\n\nYugoslavia\n8\n4\n0.6397429"
  },
  {
    "objectID": "chapter4.html#data-description",
    "href": "chapter4.html#data-description",
    "title": "Chapter 4. Trade in Intermediate Inputs and Wages",
    "section": "Data Description",
    "text": "Data Description\nHere are some variable definitions in data file data_Chp4 to help you in the replication exercise. The variable names also give you a hint as to the naming conventions used by Feenstra & Hanson with their other variables.\n\nSources\n\nNBER productivity dataset (Bartelsman, Becker, Gray):\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsic\nStandard Industrial Classification (4-digit manufacturing)\n\n\nyear\nYear ranges from 58 to 97\n\n\nemp\nTotal employment in 1000s\n\n\npay\nTotal payroll in $1,000,000\n\n\nprode\nProduction workers in 1000s\n\n\nprodh\nProduction worker hours in 1,000,000\n\n\nprodw\nProduction worker wages in $1,000,000\n\n\nvship\nTotal value of shipments in $1,000,000\n\n\nmaterial\nTotal cost of materials in $1,000,000\n\n\nvadd\nTotal value added in $1,000,000\n\n\ninvest\nTotal capital expenditure in $1,000,000\n\n\ninvent\nEnd-of-year inventories in $1,000,000\n\n\nenergy\nCost of electric & fuels in $1,000,000\n\n\ncap\nTotal real capital stock in $1,000,000\n\n\nequip\nReal capital: equipment in $1,000,000\n\n\nplant\nReal capital: structures in $1,000,000\n\n\npiship\nDeflator for VSHIP 1987=1.000\n\n\npimat\nDeflator for MATCOST 1987=1.000\n\n\npiinv\nDeflator for INVEST 1987=1.000\n\n\npien\nDeflator for ENERGY 1987=1.000\n\n\n\n\n\nOther variables created by Feenstra and Hanson:\nThe prefix “a” generally denotes an average of that variable over two periods.\nThe prefix “d” indicates annual average change in that variable x 100.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nsic72\n4 digit SIC code\n\n\nsic2\n2 digit SIC code\n\n\nptfp\nprimary TFP\n\n\nerr\nerror as defined in (4.26) of Chapter 4, or (3) in Feenstra and Hanson (1999)\n\n\nsimat1a\nShare of imported materials (broad outsourcing)\n\n\nsimat1b\nshare of imported materials from inside 2-digit industry (narrow outsourcing)\n\n\ndiffout\n= simat1a – simat1b = share of imported materials from outside 2-digit industry\n\n\nimat\nimported materials\n\n\namsh\naverage material share\n\n\naosh\naverage energy share\n\n\nadlhw\nannual change in log production wage\n\n\nadlnw\nannual chage in log non-production wage\n\n\nadlpk\nannual change in log capital price\n\n\namesh\naosh + amsh\n\n\napsh\naverage production share\n\n\nansh\naverage non-production share\n\n\naksh\naverage capital share\n\n\nnwsh\nnonproduction share of the total wages\n\n\ndlpvad\nchange in log value-added\n\n\ndlpmx\nchange in log material price\n\n\ndlpe\nchange in log energy price\n\n\ndlp\nchange in log price\n\n\ndlky\nchange in log capital stock to real shipments ratio\n\n\ndly\nchange in log real shipments\n\n\nmvshipsh\nindustry share of total manufacturing shipments, averaged over the first and last period\n\n\ndsimat1b\nchange in outsourcing (narrow); that is, change in imported inputs from the same 2-digit industry divided by total materials purchases\n\n\ndsimat1a\nchange in outsourcing (broad); that is, imported inputs divided by total material purchases\n\n\ndofsh\nchange in office equipment/total capital (capital=pstk x ex post rental price)\n\n\ndofsh1\nchange in office equipment/total capital (capital=pstk x ex ante rental price)\n\n\ndhtsh\nchange in High-tech capital/total capital (capital=pstk x ex post rental price)\n\n\ndhtsh1\nchange in High-tech capital/total capital (capital=pstk x ex ante rental price)\n\n\nci\ncomputer investment/total investment"
  },
  {
    "objectID": "chapter4.html#exercise-1",
    "href": "chapter4.html#exercise-1",
    "title": "Chapter 4. Trade in Intermediate Inputs and Wages",
    "section": "Exercise 1",
    "text": "Exercise 1\nDownload the NBER productivity dataset at http://www.nber.org/nberces/nbprod96.htm, compute the relative wage and relative employment for 1958 – 1996, and reconstruct Figure 4.1 and 4.2.\nNote: Given this data, you need to first compute the wage rates in production and nonproduction sectors using the following formula (\\(i\\) denotes the industry):\n\\[\\begin{align}\n  \\text{Production worker wage rate} &= \\frac{\\sum_i \\text{production worker wage bill}_i}{\\sum_i \\text{production workers}_i} \\\\\n  \\text{Non production worker wage rate} &= \\frac{\\sum_i \\text{non production worker wage bill}_i}{\\sum_i \\text{non production workers}_i} \\\\\n  &= \\frac{\\sum_i (\\text{total pay roll}_i - \\text{production worker wage bill}_i)}{\\sum_i (\\text{total employment}_i - \\text{production workers}_i)}\n\\end{align}\\]\n\nFeenstra’s code\nset mem 300m\n\nlog using c:\\Empirical_Exercise\\Chapter_4\\log_4_2.log,replace\n\nuse c:\\Empirical_Exercise\\Chapter_4\\data_Chp4,clear\ndrop if year==1972|year==1987\ndrop if sic72==2067|sic72==2794|sic72==3483\n\negen wagebill=sum(pay), by(year)\ngen share=pay/wagebill\n\nsort sic72 year\nby sic72: gen lagshare=share[_n-1]\ngen ashare=(share+lagshare)/2\n\nby sic72: gen lagnwsh=nwsh[_n-1]\ngen chanwsh=(nwsh-lagnwsh)*100/11\n\ngen wchanwsh=chanwsh*ashare\ngen wdlky=dlky*ashare\ngen wdly=dly*ashare\ngen wdsimat1a=dsimat1a*ashare\ngen wdsimat1b=dsimat1a*ashare\ngen diffout=dsimat1a-dsimat1b\ngen wdiffout=(dsimat1a-dsimat1b)*ashare\ngen wcosh_exp=dofsh*ashare\ngen htsh_exp=dhtsh-dofsh\ngen whtsh_exp=(dhtsh-dofsh)*ashare\ngen wcosh_exa=dofsh1*ashare\ngen htsh_exa=dhtsh1-dofsh1\ngen whtsh_exa=(dhtsh1-dofsh1)*ashare\ngen wcosh=ci*ashare\ngen whtsh=dhtsh*ashare\n\n* Check with the first column of Table 4.4 *\n\ntabstat wchanwsh wdlky wdly wdsimat1a wcosh_exp whtsh_exp wcosh_exa whtsh_exa wcosh whtsh, stats(sum)\n\n* Reproduce the rest of the columns in Table 4.4 *\n\nregress chanwsh dlky dly dsimat1a dofsh htsh_exp [aw=ashare], cluster (sic2)\n\nregress chanwsh dlky dly dsimat1a dofsh1 htsh_exa [aw=ashare], cluster (sic2)\n\nregress chanwsh dlky dly dsimat1a ci dhtsh [aw=ashare], cluster (sic2)\n\n* To instead distinguish narrow and other outsourcing, we can reproduce column (1) of table III in Feenstra and Hanson, 1999 *\n\ntabstat wchanwsh wdlky wdly wdsimat1b wdiffout wcosh_exp whtsh_exp wcosh_exa whtsh_exa wcosh whtsh, stats(sum)\n\n* Reproduce the rest of the columns in Table III *\n\nregress chanwsh dlky dly dsimat1b diffout dofsh htsh_exp [aw=ashare], cluster (sic2)\n\nregress chanwsh dlky dly dsimat1b diffout dofsh1 htsh_exa [aw=ashare], cluster (sic2)\n\nregress chanwsh dlky dly dsimat1b diffout ci dhtsh [aw=ashare], cluster (sic2)\n\nlog close\n\nclear\nexit\n\n\nMy code\n\n# Packages ----\n\nlibrary(archive)\nlibrary(haven)\nlibrary(dplyr)\nlibrary(lmtest)\nlibrary(sandwich)\n\n# Extract ----\n\nfzip &lt;- \"first-edition/Chapter-4.zip\"\ndout &lt;- gsub(\"\\\\.zip$\", \"\", fzip)\n\nif (!dir.exists(dout)) {\n  archive_extract(fzip, dir = dout)\n}\n\n# Read and transform ----\n\nfout &lt;- paste0(dout, \"/datachp4.rds\")\n\nif (!file.exists(fout)) {\n  datachp4 &lt;- read_dta(paste0(dout, \"/data_Chp4.dta\"))\n  saveRDS(datachp4, fout)\n} else {\n  datachp4 &lt;- readRDS(fout) %&gt;%\n    filter(!year %in% c(1972, 1987)) %&gt;%\n    filter(!sic72 %in% c(2067, 2794, 3483)) %&gt;%\n    group_by(year) %&gt;%\n    mutate(wagebill = sum(pay)) %&gt;%\n    ungroup() %&gt;%\n    mutate(share = pay / wagebill) %&gt;%\n    arrange(sic72, year) %&gt;%\n    group_by(sic72) %&gt;%\n    mutate(\n      lagshare = lag(share),\n      ashare = (share + lagshare) / 2,\n      lagnwsh = lag(nwsh),\n      chanwsh = (nwsh - lagnwsh) * 100 / 11\n    ) %&gt;%\n    ungroup() %&gt;%\n    mutate(\n      wchanwsh = chanwsh * ashare,\n      wdlky = dlky * ashare,\n      wdly = dly * ashare,\n      wdsimat1a = dsimat1a * ashare,\n      wdsimat1b = dsimat1a * ashare,\n      diffout = dsimat1a - dsimat1b,\n      wdiffout = (dsimat1a - dsimat1b) * ashare,\n      wcosh_exp = dofsh * ashare,\n      htsh_exp = dhtsh - dofsh,\n      whtsh_exp = (dhtsh - dofsh) * ashare,\n      wcosh_exa = dofsh1 * ashare,\n      htsh_exa = dhtsh1 - dofsh1,\n      whtsh_exa = (dhtsh1 - dofsh1) * ashare,\n      wcosh = ci * ashare,\n      whtsh = dhtsh * ashare\n    )\n}\n\n# Check with the first column of Table 4.4 ----\n\ndatachp4 %&gt;%\n  select(wchanwsh:whtsh) %&gt;%\n  summarise_all(sum, na.rm = T)\n\n# A tibble: 1 × 15\n  wchanwsh wdlky  wdly wdsimat1a wdsimat1b diffout wdiffout wcosh_exp htsh_exp\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.389 0.706  1.54     0.423     0.423    206.    0.200     0.251     164.\n# ℹ 6 more variables: whtsh_exp &lt;dbl&gt;, wcosh_exa &lt;dbl&gt;, htsh_exa &lt;dbl&gt;,\n#   whtsh_exa &lt;dbl&gt;, wcosh &lt;dbl&gt;, whtsh &lt;dbl&gt;\n\n# Reproduce the rest of the columns in Table 4.4 ----\n\nreg1 &lt;- lm(\n  chanwsh ~ dlky + dly + dsimat1a + dofsh + htsh_exp,\n  data = datachp4,\n  weights = datachp4$ashare\n)\n\n# summary(reg1) # no clustered robust standard errors\ncoeftest(reg1, vcov = vcovCL(reg1, cluster = datachp4$sic2))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  0.2028764  0.0428851  4.7307 3.017e-06 ***\ndlky         0.0467948  0.0113832  4.1109 4.702e-05 ***\ndly          0.0197383  0.0063797  3.0939  0.002101 ** \ndsimat1a     0.1966658  0.0962066  2.0442  0.041527 *  \ndofsh        0.1953400  0.0915302  2.1342  0.033381 *  \nhtsh_exp    -0.0650465  0.1371193 -0.4744  0.635464    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nreg2 &lt;- lm(\n  chanwsh ~ dlky + dly + dsimat1a + dofsh1 + htsh_exa,\n  data = datachp4,\n  weights = datachp4$ashare\n)\n\ncoeftest(reg2, vcov = vcovCL(reg2, cluster = datachp4$sic2))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 0.2064394  0.0397614  5.1920 3.183e-07 ***\ndlky        0.0444529  0.0113121  3.9297 9.872e-05 ***\ndly         0.0173278  0.0062906  2.7545  0.006121 ** \ndsimat1a    0.2207528  0.0999711  2.2082  0.027746 *  \ndofsh1      0.4309753  0.1671453  2.5784  0.010248 *  \nhtsh_exa    0.0052436  0.0712031  0.0736  0.941328    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nreg3 &lt;- lm(\n  chanwsh ~ dlky + dly + dsimat1a + ci + dhtsh,\n  data = datachp4,\n  weights = datachp4$ashare\n)\n\ncoeftest(reg3, vcov = vcovCL(reg3, cluster = datachp4$sic2))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 0.1569685  0.0446895  3.5124 0.0004898 ***\ndlky        0.0399279  0.0087378  4.5696 6.353e-06 ***\ndly         0.0100379  0.0062332  1.6104 0.1080293    \ndsimat1a    0.1346024  0.0883067  1.5243 0.1281605    \nci          0.0180834  0.0066465  2.7208 0.0067708 ** \ndhtsh       0.0324624  0.0519000  0.6255 0.5319791    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# To instead distinguish narrow and other outsourcing, we can reproduce column\n# (1) of table III in Feenstra and Hanson, 1999 ----\n\ndatachp4 %&gt;%\n  select(wchanwsh:whtsh) %&gt;%\n  summarise_all(sum, na.rm = T)\n\n# A tibble: 1 × 15\n  wchanwsh wdlky  wdly wdsimat1a wdsimat1b diffout wdiffout wcosh_exp htsh_exp\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.389 0.706  1.54     0.423     0.423    206.    0.200     0.251     164.\n# ℹ 6 more variables: whtsh_exp &lt;dbl&gt;, wcosh_exa &lt;dbl&gt;, htsh_exa &lt;dbl&gt;,\n#   whtsh_exa &lt;dbl&gt;, wcosh &lt;dbl&gt;, whtsh &lt;dbl&gt;\n\n# Reproduce the rest of the columns in Table III ----\n\nreg4 &lt;- lm(\n  chanwsh ~ dlky + dly + dsimat1b + diffout + dofsh + htsh_exp,\n  data = datachp4,\n  weights = datachp4$ashare\n)\n\ncoeftest(reg4, vcov = vcovCL(reg4, cluster = datachp4$sic2))\n\n\nt test of coefficients:\n\n              Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)  0.2069450  0.0415146  4.9849 8.933e-07 ***\ndlky         0.0421152  0.0141103  2.9847  0.002997 ** \ndly          0.0178086  0.0080568  2.2104  0.027592 *  \ndsimat1b     0.2454613  0.1692732  1.4501  0.147746    \ndiffout      0.1213620  0.0457066  2.6552  0.008213 ** \ndofsh        0.2060217  0.1021206  2.0174  0.044257 *  \nhtsh_exp    -0.0392957  0.1289341 -0.3048  0.760683    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nreg5 &lt;- lm(\n  chanwsh ~ dlky + dly + dsimat1b + diffout + dofsh1 + htsh_exa,\n  data = datachp4,\n  weights = datachp4$ashare\n)\n\ncoeftest(reg5, vcov = vcovCL(reg5, cluster = datachp4$sic2))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.2137716  0.0390531  5.4739 7.41e-08 ***\ndlky        0.0408212  0.0141101  2.8930 0.004005 ** \ndly         0.0159677  0.0078375  2.0373 0.042215 *  \ndsimat1b    0.2653356  0.1751420  1.5150 0.130497    \ndiffout     0.1537718  0.0502819  3.0582 0.002363 ** \ndofsh1      0.4207269  0.1707522  2.4640 0.014123 *  \nhtsh_exa    0.0143582  0.0722300  0.1988 0.842523    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nreg6 &lt;- lm(\n  chanwsh ~ dlky + dly + dsimat1b + diffout + ci + dhtsh,\n  data = datachp4,\n  weights = datachp4$ashare\n)\n\ncoeftest(reg6, vcov = vcovCL(reg6, cluster = datachp4$sic2))\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 0.1612801  0.0401323  4.0187 6.883e-05 ***\ndlky        0.0331274  0.0119999  2.7606  0.006010 ** \ndly         0.0068629  0.0087795  0.7817  0.434811    \ndsimat1b    0.1928058  0.1657117  1.1635  0.245257    \ndiffout     0.0380044  0.0539983  0.7038  0.481925    \nci          0.0186984  0.0068931  2.7126  0.006937 ** \ndhtsh       0.0519438  0.0512489  1.0136  0.311350    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "chapter4.html#exercise-2",
    "href": "chapter4.html#exercise-2",
    "title": "Chapter 4. Trade in Intermediate Inputs and Wages",
    "section": "Exercise 2",
    "text": "Exercise 2\nRun the STATA program Problem_4_2.do to reproduce the regressions in Table 4.4 (which is simplified from Table III in Feenstra and Hanson, 1999). Then answer:\n\nWhat weights are used in these regressions?\nHow are the results affected if these weights are not used?\n\n\nFeenstra’s code\nset mem 3m\n\nlog using c:\\Empirical_Exercise\\Chapter_4\\log_4_3a.log,replace\n\nuse c:\\Empirical_Exercise\\Chapter_4\\data_Chp4.dta, clear\n\nkeep if year==1990\ndrop if sic72==2067\ndrop if sic72==2794\ndrop if sic72==3483\ngen etfp=ptfp-err\ngen adj1=1/(1-amesh)\ngen etfp1=adj1*etfp\ngen dlpvad1=adj1*dlpvad\ngen apsh1=adj1*apsh\ngen ansh1=adj1*ansh\ngen aksh1=adj1*aksh\ngen mshxpr=amsh*dlpmx\ngen eshxpr=aosh*dlpe\n\n* Reproduce Table 4.5 *\n\ngen dlp34=dlp-mshxpr-eshxpr\n\nregress dlp34 ptfp apsh ansh aksh [aw=mvshipsh], robust\n\npreserve\ndrop if sic72==3573\nregress dlp34 ptfp apsh ansh aksh [aw=mvshipsh], robust\n\nregress dlp apsh ansh aksh mshxpr eshxpr [aw=mvshipsh], robust\nrestore\n\nregress dlpvad1 etfp1 apsh1 ansh1 aksh1 [aw=mvshipsh],robust noconstant\n\nregress dlp etfp apsh ansh aksh mshxpr eshxpr [aw=mvshipsh], robust\n\nlog close\nclear\nexit\n\n\nMy code\n\n# Read and transform ----\n\ndatachp4 &lt;- readRDS(fout) %&gt;%\n  filter(year == 1990) %&gt;%\n  filter(!sic72 %in% c(2067, 2794, 3483)) %&gt;%\n  mutate(\n    etfp = ptfp - err,\n    adj1 = 1 / (1 - amesh),\n    etfp1 = adj1 * etfp,\n    dlpvad1 = adj1 * dlpvad,\n    apsh1 = adj1 * apsh,\n    ansh1 = adj1 * ansh,\n    aksh1 = adj1 * aksh,\n    mshxpr = amsh * dlpmx,\n    eshxpr = aosh * dlpe\n  )\n\n# Reproduce Table 4.5 ----\n\ndatachp4 &lt;- datachp4 %&gt;%\n  mutate(dlp34 = dlp - mshxpr - eshxpr)\n\nreg1 &lt;- lm(\n  dlp34 ~ ptfp + apsh + ansh + aksh,\n  data = datachp4,\n  weights = datachp4$mvshipsh\n)\n\n# HC1 is the Stata default\ncoeftest(reg1, vcov = vcovHC(reg1, type = \"HC1\"))\n\n\nt test of coefficients:\n\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) -0.705112   0.300602  -2.3457  0.01943 *  \nptfp        -0.963182   0.070209 -13.7187  &lt; 2e-16 ***\napsh         3.062598   1.221980   2.5063  0.01256 *  \nansh         2.294716   1.430073   1.6046  0.10929    \naksh         7.887571   0.781001  10.0993  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# there is no equivalent to the Stata command \"preserve\" in R\n# therefore, I make a copy of the data and drop the observations\n\ndatachp4_2 &lt;- datachp4 %&gt;%\n  filter(sic72 != 3573)\n\nreg2 &lt;- lm(\n  dlp34 ~ ptfp + apsh + ansh + aksh,\n  data = datachp4_2,\n  weights = datachp4_2$mvshipsh\n)\n\ncoeftest(reg2, vcov = vcovHC(reg2, type = \"HC1\"))\n\n\nt test of coefficients:\n\n             Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) -0.824927   0.293099  -2.8145  0.005104 ** \nptfp        -0.753115   0.075189 -10.0163 &lt; 2.2e-16 ***\napsh         2.427856   1.162844   2.0879  0.037384 *  \nansh         4.086394   1.722144   2.3729  0.018079 *  \naksh         8.058291   0.941170   8.5620 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nreg3 &lt;- lm(\n  dlp ~ apsh + ansh + aksh + mshxpr + eshxpr,\n  data = datachp4_2,\n  weights = datachp4_2$mvshipsh\n)\n\ncoeftest(reg3, vcov = vcovHC(reg3, type = \"HC1\"))\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) -1.92919    0.91478 -2.1089   0.03552 *  \napsh         3.60528    1.88524  1.9124   0.05648 .  \nansh         6.20267    4.03647  1.5367   0.12510    \naksh         9.53521    2.18722  4.3595 1.625e-05 ***\nmshxpr       1.21930    0.24713  4.9338 1.146e-06 ***\neshxpr      -0.93012    0.91503 -1.0165   0.30995    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nreg4 &lt;- lm(\n  dlpvad1 ~ etfp1 + apsh1 + ansh1 + aksh1 + 0,\n  data = datachp4,\n  weights = datachp4$mvshipsh\n)\n\ncoeftest(reg4, vcov = vcovHC(reg4, type = \"HC1\"))\n\n\nt test of coefficients:\n\n         Estimate  Std. Error  t value  Pr(&gt;|t|)    \netfp1 -1.00004119  0.00068315 -1463.87 &lt; 2.2e-16 ***\napsh1  4.68065661  0.01577181   296.77 &lt; 2.2e-16 ***\nansh1  5.48280782  0.01946769   281.64 &lt; 2.2e-16 ***\naksh1  3.95253801  0.00834071   473.88 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# regress dlp etfp apsh ansh aksh mshxpr eshxpr [aw=mvshipsh], robust\n\nreg5 &lt;- lm(\n  dlp ~ etfp + apsh + ansh + aksh + mshxpr + eshxpr,\n  data = datachp4,\n  weights = datachp4$mvshipsh\n)\n\ncoeftest(reg5, vcov = vcovHC(reg5, type = \"HC1\"))\n\n\nt test of coefficients:\n\n               Estimate  Std. Error    t value Pr(&gt;|t|)    \n(Intercept)  0.00107993  0.00542304     0.1991   0.8422    \netfp        -1.00035789  0.00067704 -1477.5556   &lt;2e-16 ***\napsh         4.70001239  0.01191103   394.5931   &lt;2e-16 ***\nansh         5.44331510  0.03144047   173.1308   &lt;2e-16 ***\naksh         3.97230835  0.01502835   264.3210   &lt;2e-16 ***\nmshxpr       0.99740722  0.00231147   431.5032   &lt;2e-16 ***\neshxpr       0.99611082  0.00574213   173.4741   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "chapter4.html#exercise-3",
    "href": "chapter4.html#exercise-3",
    "title": "Chapter 4. Trade in Intermediate Inputs and Wages",
    "section": "Exercise 3",
    "text": "Exercise 3\nRun the STATA program Problem_4_3a.do to reproduce the regressions in Table 4.5 (i.e. Table I in Feenstra and Hansen, 1999). Then run Problem_4_3b.do to perform the two-step regression, Table IV and Table V in Feenstra and Hanson (1999). Note that Table V is obtained using the coefficients in the first column of Table IV.\n\nFeenstra’s code\nset mem 3m\ncapture log close\nlog using c:\\Empirical_Exercise\\Chapter_4\\log_4_3b.log,replace\n\nuse c:\\Empirical_Exercise\\Chapter_4\\data_Chp4, clear\n\nkeep if year==1990\ndrop if sic72==2067\ndrop if sic72==2794\ndrop if sic72==3483\ngen etfp=ptfp-err\ngen adj1=1/(1-amesh)\ngen etfp1=adj1*etfp\ngen dlpvad1=adj1*dlpvad\ngen apsh1=adj1*apsh\ngen ansh1=adj1*ansh\ngen aksh1=adj1*aksh\ngen t4dlpvad=(dlpvad+etfp)*adj1\npreserve\n\n* Reproduce the first column of Table IV  *\n* generating difference measure of outsourcing *\n\ngen dsimatd1=dsimat1a-dsimat1b\n\n* generating difference measure of high tech share *\n\ngen dhtdsh=dhtsh-dofsh\n\n* check whether we are using the right variable as described in table II *\n\nsum dsimatd1 dhtdsh dofsh [aw=mvshipsh]\n\nregress t4dlpvad dsimat1b dsimatd1 dofsh dhtdsh [aw=mvshipsh], cluster(sic2)\n\n* Reproduce Table V using the coefficients in column(1) of Table IV *\n\ngen wt=mvshipsh^.5\ngen apsh5=apsh1*wt\ngen ansh5=ansh1*wt\ngen aksh5=aksh1*wt\ngen narrout=dsimat1b*wt*_coef[dsimat1b]\ngen diffout=dsimatd1*wt*_coef[dsimatd1]\ngen comsh=dofsh*wt*_coef[dofsh]\ngen difcom=dhtdsh*wt*_coef[dhtdsh]\n\nsum narrout diffout comsh difcom\n\nregress narrout apsh5 ansh5 aksh5, noconstant\nregress diffout apsh5 ansh5 aksh5, noconstant\nregress comsh apsh5 ansh5 aksh5, noconstant\nregress difcom apsh5 ansh5 aksh5, noconstant\n\nrestore\n\n* Reproduce column (2) of Table IV *\n\npreserve\n\n* generating difference measure of outsourcing *\n\ngen dsimatd1=dsimat1a-dsimat1b\n\n* generate difference measure of high tech share with ex ante rental price *\n\ngen dhtdsh1=dhtsh1-dofsh1\n\n* check whether we are using the right variable as described in table II *\n\nsum dsimatd1 dhtdsh1 dofsh1 [aw=mvshipsh]\n\nregress t4dlpvad dsimat1b dsimatd1 dofsh1 dhtdsh1 [aw=mvshipsh], cluster(sic2)\n\n* Reproduce column (3) of Table IV *\n\n* generating difference measure of high tech share *\n\ngen dhtdsh=dhtsh-dofsh\n\nregress t4dlpvad dsimat1b dsimatd1 ci dhtsh [aw=mvshipsh], cluster(sic2)\n\nlog close\nclear\n\nexit\n\n\nMy code\n\n# Packages ----\n\nlibrary(tidyr)\n\n# Read and transform ----\n\ndatachp4 &lt;- readRDS(fout) %&gt;%\n  filter(year == 1990) %&gt;%\n  filter(!sic72 %in% c(2067, 2794, 3483)) %&gt;%\n  mutate(\n    etfp = ptfp - err,\n    adj1 = 1 / (1 - amesh),\n    etfp1 = adj1 * etfp,\n    dlpvad1 = adj1 * dlpvad,\n    apsh1 = adj1 * apsh,\n    ansh1 = adj1 * ansh,\n    aksh1 = adj1 * aksh,\n    t4dlpvad = (dlpvad + etfp) * adj1\n  )\n\n# Reproduce the first column of Table IV\n# generating difference measure of outsourcing ----\n\ndatachp4_2 &lt;- datachp4 %&gt;%\n  mutate(dsimatd1 = dsimat1a - dsimat1b)\n\n# Generating difference measure of high tech share ----\n\ndatachp4_2 &lt;- datachp4_2 %&gt;%\n  mutate(dhtdsh = dhtsh - dofsh)\n\n# Check whether we are using the right variable as described in table II ----\n\n# sum dsimatd1 dhtdsh dofsh [aw=mvshipsh] is particularly hard to replicate\n\ndatachp4_2_1 &lt;- datachp4_2 %&gt;%\n  select(dsimatd1, dhtdsh, dofsh) %&gt;%\n  summarise_all(list(nobs = length, min = min, max = max, sd = sd)) %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;%\n  separate(variable, into = c(\"var\", \"stat\"), sep = \"_\") %&gt;%\n  pivot_wider(\n    names_from = stat,\n    values_from = value\n  )\n\ndatachp4_2_2 &lt;- datachp4_2 %&gt;%\n  summarise(weight = sum(mvshipsh)) %&gt;%\n  bind_cols(var = c(\"dsimatd1\", \"dhtdsh\", \"dofsh\"))\n\ndatachp4_2_3 &lt;- datachp4_2 %&gt;%\n  mutate(across(c(dsimatd1, dhtdsh, dofsh), ~ . * mvshipsh)) %&gt;%\n  select(dsimatd1, dhtdsh, dofsh) %&gt;%\n  summarise_all(list(wsum = sum)) %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;%\n  separate(variable, into = c(\"var\", \"stat\"), sep = \"_\") %&gt;%\n  pivot_wider(\n    names_from = stat,\n    values_from = value\n  )\n\ndatachp4_2_1 %&gt;%\n  left_join(datachp4_2_2) %&gt;%\n  left_join(datachp4_2_3)\n\n# A tibble: 3 × 7\n  var       nobs     min   max    sd weight  wsum\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 dsimatd1   447 -1.76   2.74  0.340  0.999 0.160\n2 dhtdsh     447 -0.0842 0.974 0.219  0.999 0.128\n3 dofsh      447 -0.363  0.831 0.308  0.999 0.198\n\nreg1 &lt;- lm(\n  t4dlpvad ~ dsimat1b + dsimatd1 + dofsh + dhtdsh,\n  data = datachp4_2,\n  weights = datachp4_2$mvshipsh\n)\n\ncoeftest(reg1, vcov = vcovCL(reg1, cluster = datachp4_2$sic2))\n\n\nt test of coefficients:\n\n            Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) 4.262727   0.032292 132.0067  &lt; 2e-16 ***\ndsimat1b    0.063503   0.030585   2.0763  0.03845 *  \ndsimatd1    0.078814   0.047216   1.6692  0.09578 .  \ndofsh       0.166569   0.065895   2.5278  0.01182 *  \ndhtdsh      0.075982   0.072249   1.0517  0.29353    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Reproduce Table V using the coefficients in column(1) of Table IV ----\n\ndatachp4_2 &lt;- datachp4_2 %&gt;%\n  mutate(\n    wt = sqrt(mvshipsh),\n    apsh5 = apsh1 * wt,\n    ansh5 = ansh1 * wt,\n    aksh5 = aksh1 * wt,\n    narrout = dsimat1b * wt * coef(reg1)[\"dsimat1b\"],\n    diffout = dsimatd1 * wt * coef(reg1)[\"dsimatd1\"],\n    comsh = dofsh * wt * coef(reg1)[\"dofsh\"],\n    difcom = dhtdsh * wt * coef(reg1)[\"dhtdsh\"]\n  )\n\ndatachp4_2 %&gt;%\n  select(narrout:difcom) %&gt;%\n  summarise_all(\n    list(nobs = length, mean = mean, sd = sd, min = min, max = max)\n  ) %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;%\n  separate(variable, into = c(\"var\", \"stat\"), sep = \"_\") %&gt;%\n  pivot_wider(\n    names_from = stat,\n    values_from = value\n  )\n\n# A tibble: 4 × 6\n  var      nobs     mean       sd       min     max\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 narrout   447 0.000411 0.00128  -0.00777  0.0132 \n2 diffout   447 0.000555 0.00122  -0.00540  0.0157 \n3 comsh     447 0.00125  0.00214  -0.00285  0.0110 \n4 difcom    447 0.000404 0.000739 -0.000935 0.00643\n\nreg2 &lt;- lm(\n  narrout ~ apsh5 + ansh5 + aksh5 + 0,\n  data = datachp4_2\n)\n\nsummary(reg2)\n\n\nCall:\nlm(formula = narrout ~ apsh5 + ansh5 + aksh5 + 0, data = datachp4_2)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0083329 -0.0004728 -0.0002127  0.0000644  0.0119091 \n\nCoefficients:\n       Estimate Std. Error t value Pr(&gt;|t|)    \napsh5 -0.009516   0.009351  -1.018    0.309    \nansh5  0.098667   0.014774   6.678 7.25e-11 ***\naksh5  0.002638   0.003536   0.746    0.456    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.001161 on 444 degrees of freedom\nMultiple R-squared:  0.2611,    Adjusted R-squared:  0.2561 \nF-statistic: 52.29 on 3 and 444 DF,  p-value: &lt; 2.2e-16\n\nreg3 &lt;- lm(\n  diffout ~ apsh5 + ansh5 + aksh5 + 0,\n  data = datachp4_2\n)\n\nsummary(reg3)\n\n\nCall:\nlm(formula = diffout ~ apsh5 + ansh5 + aksh5 + 0, data = datachp4_2)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0063691 -0.0003192 -0.0000461  0.0003706  0.0133647 \n\nCoefficients:\n       Estimate Std. Error t value Pr(&gt;|t|)    \napsh5  0.020364   0.009476   2.149   0.0322 *  \nansh5  0.062848   0.014972   4.198 3.26e-05 ***\naksh5 -0.001140   0.003583  -0.318   0.7506    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.001177 on 444 degrees of freedom\nMultiple R-squared:  0.2317,    Adjusted R-squared:  0.2266 \nF-statistic: 44.65 on 3 and 444 DF,  p-value: &lt; 2.2e-16\n\nreg4 &lt;- lm(\n  comsh ~ apsh5 + ansh5 + aksh5 + 0,\n  data = datachp4_2\n)\n\nsummary(reg4)\n\n\nCall:\nlm(formula = comsh ~ apsh5 + ansh5 + aksh5 + 0, data = datachp4_2)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0051922 -0.0009209 -0.0003753  0.0007447  0.0074504 \n\nCoefficients:\n        Estimate Std. Error t value Pr(&gt;|t|)    \napsh5 -0.0049722  0.0140295  -0.354    0.723    \nansh5  0.2480142  0.0221662  11.189   &lt;2e-16 ***\naksh5  0.0007009  0.0053051   0.132    0.895    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.001742 on 444 degrees of freedom\nMultiple R-squared:  0.5086,    Adjusted R-squared:  0.5053 \nF-statistic: 153.2 on 3 and 444 DF,  p-value: &lt; 2.2e-16\n\nreg5 &lt;- lm(\n  difcom ~ apsh5 + ansh5 + aksh5 + 0,\n  data = datachp4_2\n)\n\nsummary(reg5)\n\n\nCall:\nlm(formula = difcom ~ apsh5 + ansh5 + aksh5 + 0, data = datachp4_2)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0025959 -0.0002751 -0.0000947  0.0001466  0.0058306 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)    \napsh5 0.025945   0.005624   4.613  5.2e-06 ***\nansh5 0.006921   0.008886   0.779   0.4364    \naksh5 0.004331   0.002127   2.036   0.0423 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0006985 on 444 degrees of freedom\nMultiple R-squared:  0.3149,    Adjusted R-squared:  0.3103 \nF-statistic: 68.02 on 3 and 444 DF,  p-value: &lt; 2.2e-16\n\n# Reproduce column (2) of Table IV ----\n\n## Generating difference measure of outsourcing ----\n\ndatachp4 &lt;- datachp4 %&gt;%\n  mutate(dsimatd1 = dsimat1a - dsimat1b)\n\n## Generate difference measure of high tech share with ex ante rental price ----\n\ndatachp4 &lt;- datachp4 %&gt;%\n  mutate(dhtdsh1 = dhtsh1 - dofsh1)\n\n## Check whether we are using the right variable as described in table II ----\n\ndatachp4 %&gt;%\n  select(dsimatd1, dhtdsh1, dofsh1) %&gt;%\n  summarise_all(list(nobs = length, min = min, max = max, sd = sd)) %&gt;%\n  pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;%\n  separate(variable, into = c(\"var\", \"stat\"), sep = \"_\") %&gt;%\n  pivot_wider(\n    names_from = stat,\n    values_from = value\n  ) %&gt;%\n  left_join(\n    datachp4 %&gt;%\n      summarise(weight = sum(mvshipsh)) %&gt;%\n      bind_cols(var = c(\"dsimatd1\", \"dhtdsh1\", \"dofsh1\"))\n  ) %&gt;%\n  left_join(\n    datachp4 %&gt;%\n      mutate(across(c(dsimatd1, dhtdsh1, dofsh1), ~ . * mvshipsh)) %&gt;%\n      select(dsimatd1, dhtdsh1, dofsh1) %&gt;%\n      summarise_all(list(wsum = sum)) %&gt;%\n      pivot_longer(\n        cols = everything(),\n        names_to = \"variable\",\n        values_to = \"value\"\n      ) %&gt;%\n      separate(variable, into = c(\"var\", \"stat\"), sep = \"_\") %&gt;%\n      pivot_wider(\n        names_from = stat,\n        values_from = value\n      )\n  )\n\n# A tibble: 3 × 7\n  var       nobs     min   max    sd weight   wsum\n  &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 dsimatd1   447 -1.76   2.74  0.340  0.999 0.160 \n2 dhtdsh1    447  0.0204 0.900 0.171  0.999 0.164 \n3 dofsh1     447 -0.270  0.380 0.152  0.999 0.0534\n\n# regress t4dlpvad dsimat1b dsimatd1 dofsh1 dhtdsh1 [aw=mvshipsh], cluster(sic2)\n\nreg2 &lt;- lm(\n  t4dlpvad ~ dsimat1b + dsimatd1 + dofsh1 + dhtdsh1,\n  data = datachp4,\n  weights = datachp4$mvshipsh\n)\n\ncoeftest(reg2, vcov = vcovCL(reg2, cluster = datachp4$sic2))\n\n\nt test of coefficients:\n\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)  4.294261   0.038595 111.2650  &lt; 2e-16 ***\ndsimat1b     0.079517   0.034676   2.2931  0.02231 *  \ndsimatd1     0.113680   0.044020   2.5825  0.01013 *  \ndofsh1       0.192416   0.108362   1.7757  0.07647 .  \ndhtdsh1     -0.047795   0.082049  -0.5825  0.56052    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Reproduce column (3) of Table IV ----\n\n## Generating difference measure of high tech share ----\n\ndatachp4 &lt;- datachp4 %&gt;%\n  mutate(dhtdsh = dhtsh - dofsh)\n\nreg3 &lt;- lm(\n  t4dlpvad ~ dsimat1b + dsimatd1 + ci + dhtsh,\n  data = datachp4,\n  weights = datachp4$mvshipsh\n)\n\ncoeftest(reg3, vcov = vcovCL(reg3, cluster = datachp4$sic2))\n\n\nt test of coefficients:\n\n             Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept) 4.2438613  0.0334856 126.7368  &lt; 2e-16 ***\ndsimat1b    0.0404060  0.0295213   1.3687  0.17179    \ndsimatd1    0.0351687  0.0488208   0.7204  0.47168    \nci          0.0081792  0.0045064   1.8150  0.07020 .  \ndhtsh       0.0930741  0.0496036   1.8764  0.06126 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "chapter5.html#documentation",
    "href": "chapter5.html#documentation",
    "title": "Chapter 5. Increasing Returns and the Gravity Equation",
    "section": "Documentation",
    "text": "Documentation\nUS-Canada data for Anderson and van Wincoop (2002)\nThere are a total of 63 US-Canada regions (states, District of Columbia, provinces and territories). They are listed below. The regressions, however, are based on the same 40 states and provinces as in McCallum (these are indicated with a star below).\n\nAlabama*\nAlaska\n\nArizona*\nArkansas\n\nCalifornia*\nColorado\n\nConnecticut\nDelaware\n\nFlorida*\nGeorgia*\nHawaii\n\nIdaho*\nIllinois*\nIndiana*\nIowa\n\nKansas\n\nKentucky*\nLouisiana*\nMaine*\nMaryland*\n\nMassachusetts*\nMichigan*\n\nMinnesota*\n\nMississippi\n\nMissouri*\n\nMontana*\n\nNebraska\n\nNevada\n\nNew Hampshire*\nNew Jersey*\n\nNew Mexico\n\nNew York*\n\nNorth Carolina*\nNorth Dakota*\n\nOhio*\n\nOklahoma\n\nOregon\n\nPennsylvania*\nRhode Island\nSouth Carolina\n\nSouth Dakota\n\nTennessee*\n\nTexas*\n\nUtah\n\nVermont*\n\nVirginia*\n\nWashington*\n\nWest Virginia\n\nWisconsin*\n\nWyoming\n\nDist. of Col.\n\nAlberta*\n\nBritish Columbia*\nManitoba*\n\nNew Brunswick *\nNewfoundland*\nNW Territories\nNova Scotia*\n\nOntario*\n\nPrince Edward Island*\nQuebec*\n\nSaskatchewan*\nYukon Territory\n\nData files:\n\ndist.csv: Contains distances between the 40 regions listed above. The distances are in kilometers and are between the capitals of the regions.\ngdp_ce_93.csv and gdp_ci_93.csv: Contains nominal GDP in millions of Canadian dollars in 1993 for the 40 regions above.\ntrade_93.csv\n\nContains 1993 trade data between the 40 regions listed above, in US dollars. The indicator variables “1_ex” and “1_im” equal 1 if the exporter or importer is a US state, and 2 for a Canadian province."
  },
  {
    "objectID": "chapter5.html#empirical-exercise",
    "href": "chapter5.html#empirical-exercise",
    "title": "Chapter 5. Increasing Returns and the Gravity Equation",
    "section": "Empirical exercise",
    "text": "Empirical exercise\nIn this exercise, you are asked to reproduce the empirical results shown in Table 5.2. There are four datasets available: “dist.csv” which is distances; “gdp_ce_93.csv” which is GDP in exporting location in 1993; “gdp_ci_93.csv” which is GDP in importing location in 1993; and “trade_93.csv” which is trade in 1993. To complete the exercise, these files should be stored in the directory: c:_Exercise_5. After this, run the STATA program “data_trans.do,” which will convert these datasets to STATA files with the same name. The trade data is already converted into US dollars, but GDP data is in Canadian dollars, so this is converted with the exchange rate 1 Canadian dollar = 0.775134 U.S. dollars."
  },
  {
    "objectID": "chapter5.html#exercise-1",
    "href": "chapter5.html#exercise-1",
    "title": "Chapter 5. Increasing Returns and the Gravity Equation",
    "section": "Exercise 1",
    "text": "Exercise 1\nRun the program “gravity_1.do” to replicate the gravity equations in columns (1)-(3) of Table 5.2.\n\nFeenstra’s code\ncapture log close\nlog using c:\\Empirical_Exercise\\Chapter_5\\gravity_1.log, replace\n\nset matsize 100\n\nuse c:\\Empirical_Exercise\\Chapter_5\\trade_93,clear\nsort c_e\nmerge c_e using c:\\Empirical_Exercise\\Chapter_5\\gdp_ce_93\ndrop _merge\nsort c_i\nmerge c_i using c:\\Empirical_Exercise\\Chapter_5\\gdp_ci_93\ndrop _merge\ndrop if vx==0\ndrop if dist==0\n\ngen lnvx=log(vx)\ngen lndist=log(dist)\ngen lngdp_ce=log(gdp_ce)\ngen lngdp_ci=log(gdp_ci)\n\n* Estimate Gravity Equation from the Canadian Perspective *\n\npreserve\ngen d_ca=0\nreplace d_ca=1 if (l_ex==2) & (l_im==2)\ndrop if (l_ex==1) & (l_im==1)\n\nregress lnvx lngdp_ce lngdp_ci lndist d_ca\nrestore\n\n* Estimate Gravity Equation from the U.S. Perspective *\n\npreserve\ngen d_us=0\nreplace d_us=1 if (l_ex==1) & (l_im==1)\ndrop if (l_ex==2) & (l_im==2)\n\nregress lnvx lngdp_ce lngdp_ci lndist d_us\nrestore\n\n* Estimate Gravity Equation by Pooling All Data *\n\npreserve\ngen d_ca=0\ngen d_us=0\nreplace d_ca=1 if (l_ex==2) & (l_im==2)\nreplace d_us=1 if (l_ex==1) & (l_im==1)\n\nregress lnvx lngdp_ce lngdp_ci lndist d_ca d_us\nvce\nrestore\n\nclear\n\nlog close\n\n\nMy code\n\n# Packages ----\n\nlibrary(archive)\nlibrary(haven)\nlibrary(dplyr)\n\n# Extract ----\n\nfzip &lt;- \"first-edition/Chapter-5.zip\"\ndout &lt;- gsub(\"\\\\.zip$\", \"\", fzip)\n\nif (!dir.exists(dout)) {\n  archive_extract(fzip, dir = dout)\n}\n\n# Read and transform ----\n\nfout &lt;- paste0(dout, \"/trade_93.rds\")\n\nif (!file.exists(fout)) {\n  trade_93 &lt;- read_dta(paste0(dout, \"/trade_93.dta\"))\n  gdp_ce_93 &lt;- read_dta(paste0(dout, \"/gdp_ce_93.dta\"))\n  gdp_ci_93 &lt;- read_dta(paste0(dout, \"/gdp_ci_93.dta\"))\n\n  trade_93 &lt;- trade_93 %&gt;%\n    left_join(gdp_ce_93, by = \"c_e\") %&gt;%\n    left_join(gdp_ci_93, by = \"c_i\") %&gt;%\n    filter(vx != 0, dist != 0) %&gt;%\n    mutate(\n      lnvx = log(vx),\n      lndist = log(dist),\n      lngdp_ce = log(gdp_ce),\n      lngdp_ci = log(gdp_ci)\n    )\n\n  saveRDS(trade_93, fout)\n} else {\n  trade_93 &lt;- readRDS(fout)\n}\n\n# Estimate Gravity Equation from the Canadian Perspective ----\n\ntrade_93_2 &lt;- trade_93 %&gt;%\n  mutate(d_ca = ifelse(l_ex == 2 & l_im == 2, 1, 0)) %&gt;%\n  filter(l_ex != 1 | l_im != 1)\n\nfit_ca &lt;- lm(lnvx ~ lngdp_ce + lngdp_ci + lndist + d_ca, data = trade_93_2)\n\nsummary(fit_ca)\n\n\nCall:\nlm(formula = lnvx ~ lngdp_ce + lngdp_ci + lndist + d_ca, data = trade_93_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9344 -0.6428  0.0174  0.6225  4.0379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.74267    0.77220   4.847 1.56e-06 ***\nlngdp_ce     1.21871    0.03316  36.754  &lt; 2e-16 ***\nlngdp_ci     0.97978    0.03253  30.124  &lt; 2e-16 ***\nlndist      -1.35315    0.06901 -19.607  &lt; 2e-16 ***\nd_ca         2.80203    0.14170  19.775  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.183 on 674 degrees of freedom\nMultiple R-squared:  0.7622,    Adjusted R-squared:  0.7608 \nF-statistic:   540 on 4 and 674 DF,  p-value: &lt; 2.2e-16\n\n# Estimate Gravity Equation from the U.S. Perspective ----\n\ntrade_93_3 &lt;- trade_93 %&gt;%\n  mutate(d_us = ifelse(l_ex == 1 & l_im == 1, 1, 0)) %&gt;%\n  filter(l_ex != 2 | l_im != 2)\n\nfit_us &lt;- lm(lnvx ~ lngdp_ce + lngdp_ci + lndist + d_us, data = trade_93_3)\n\nsummary(fit_us)\n\n\nCall:\nlm(formula = lnvx ~ lngdp_ce + lngdp_ci + lndist + d_us, data = trade_93_3)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2863 -0.4620 -0.0077  0.4822  3.7858 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.65959    0.44927   5.920 4.04e-09 ***\nlngdp_ce     1.12843    0.02045  55.172  &lt; 2e-16 ***\nlngdp_ci     0.98203    0.02040  48.148  &lt; 2e-16 ***\nlndist      -1.08189    0.03523 -30.712  &lt; 2e-16 ***\nd_us         0.40597    0.05787   7.016 3.54e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9292 on 1416 degrees of freedom\nMultiple R-squared:  0.8529,    Adjusted R-squared:  0.8525 \nF-statistic:  2053 on 4 and 1416 DF,  p-value: &lt; 2.2e-16\n\n# Estimate Gravity Equation by Pooling All Data ----\n\ntrade_93_4 &lt;- trade_93 %&gt;%\n  mutate(\n    d_ca = ifelse(l_ex == 2 & l_im == 2, 1, 0),\n    d_us = ifelse(l_ex == 1 & l_im == 1, 1, 0)\n  )\n\nfit_all &lt;- lm(\n  lnvx ~ lngdp_ce + lngdp_ci + lndist + d_ca + d_us,\n  data = trade_93_4\n)\n\nsummary(fit_all)\n\n\nCall:\nlm(formula = lnvx ~ lngdp_ce + lngdp_ci + lndist + d_ca + d_us, \n    data = trade_93_4)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2531 -0.4630 -0.0100  0.4902  3.8101 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.91151    0.42672   6.823 1.29e-11 ***\nlngdp_ce     1.13297    0.01968  57.571  &lt; 2e-16 ***\nlngdp_ci     0.97422    0.01963  49.630  &lt; 2e-16 ***\nlndist      -1.11070    0.03373 -32.925  &lt; 2e-16 ***\nd_ca         2.75171    0.10868  25.320  &lt; 2e-16 ***\nd_us         0.39827    0.05744   6.933 6.08e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9304 on 1505 degrees of freedom\nMultiple R-squared:  0.852, Adjusted R-squared:  0.8515 \nF-statistic:  1733 on 5 and 1505 DF,  p-value: &lt; 2.2e-16\n\nvcov(fit_all)\n\n             (Intercept)      lngdp_ce      lngdp_ci        lndist\n(Intercept)  0.182087517 -5.244280e-03 -5.204609e-03 -8.948498e-03\nlngdp_ce    -0.005244280  3.872922e-04  8.278679e-05  1.867887e-05\nlngdp_ci    -0.005204609  8.278679e-05  3.853139e-04  1.752375e-05\nlndist      -0.008948498  1.867887e-05  1.752375e-05  1.138030e-03\nd_ca        -0.011574809  4.124081e-04  4.010346e-04  1.704325e-04\nd_us         0.003876605 -3.748832e-04 -3.931548e-04  3.969829e-04\n                     d_ca          d_us\n(Intercept) -0.0115748092  0.0038766053\nlngdp_ce     0.0004124081 -0.0003748832\nlngdp_ci     0.0004010346 -0.0003931548\nlndist       0.0001704325  0.0003969829\nd_ca         0.0118103647  0.0008562509\nd_us         0.0008562509  0.0032996176"
  },
  {
    "objectID": "chapter5.html#exercise-2",
    "href": "chapter5.html#exercise-2",
    "title": "Chapter 5. Increasing Returns and the Gravity Equation",
    "section": "Exercise 2",
    "text": "Exercise 2\nRun the program “gravity_2.do” to replicate gravity equation using fixed-effects, i.e., column (5) in Table 5.2. Then answer:\n\nHow are these results affected if we allow the provincial and state GDP’s to have coefficients different from unity?\nWhat coefficients are obtained if we introduce separate indicator variables for intra-Canadian and intra-U.S. trade, rather than the border dummy?\n\n\nFeenstra’s code\ncapture log close\nlog using c:\\Empirical_Exercise\\Chapter_5\\gravity_2.log, replace\n\nset matsize 100\n\nuse c:\\Empirical_Exercise\\Chapter_5\\trade_93,clear\nsort c_e\nmerge c_e using c:\\Empirical_Exercise\\Chapter_5\\gdp_ce_93\ndrop _merge\nsort c_i\nmerge c_i using c:\\Empirical_Exercise\\Chapter_5\\gdp_ci_93\ndrop _merge\ndrop if vx==0\ndrop if dist==0\n\ntab c_e, gen (ced)\ntab c_i, gen (cid)\n\ngen d_border=1\nreplace d_border=0 if (l_ex==1) & (l_im==1)\nreplace d_border=0 if (l_ex==2) & (l_im==2)\n\ngen lnvx=log(vx)\ngen lndist=log(dist)\ngen lngdp_ce=log(gdp_ce)\ngen lngdp_ci=log(gdp_ci)\ngen lnn_vx=lnvx-lngdp_ce-lngdp_ci\n\nregress lnn_vx lndist d_border ced* cid*\n\nclear\nlog close\n\n\nMy code\n\ntrade_93 &lt;- readRDS(paste0(dout, \"/trade_93.rds\"))\n\ntrade_93 %&gt;%\n  group_by(c_e) %&gt;%\n  summarise(\n    freq = n(),\n    percent = 100 * n() / nrow(trade_93)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(cum = cumsum(percent))\n\n# A tibble: 40 × 4\n   c_e    freq percent   cum\n   &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 AB       39    2.58  2.58\n 2 Ala      38    2.51  5.10\n 3 Ari      37    2.45  7.54\n 4 BC       39    2.58 10.1 \n 5 Cal      37    2.45 12.6 \n 6 Flo      39    2.58 15.2 \n 7 Geo      39    2.58 17.7 \n 8 Ida      36    2.38 20.1 \n 9 Ill      39    2.58 22.7 \n10 Ind      38    2.51 25.2 \n# ℹ 30 more rows\n\ntrade_93 %&gt;%\n  group_by(c_i) %&gt;%\n  summarise(\n    freq = n(),\n    percent = 100 * n() / nrow(trade_93)\n  ) %&gt;%\n  ungroup() %&gt;%\n  mutate(cum = cumsum(percent))\n\n# A tibble: 40 × 4\n   c_i    freq percent   cum\n   &lt;chr&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 AB       39    2.58  2.58\n 2 Ala      38    2.51  5.10\n 3 Ari      36    2.38  7.48\n 4 BC       39    2.58 10.1 \n 5 Cal      39    2.58 12.6 \n 6 Flo      39    2.58 15.2 \n 7 Geo      39    2.58 17.8 \n 8 Ida      33    2.18 20.0 \n 9 Ill      39    2.58 22.6 \n10 Ind      39    2.58 25.1 \n# ℹ 30 more rows\n\n# gen d_border=1\n# replace d_border=0 if (l_ex==1) & (l_im==1)\n# replace d_border=0 if (l_ex==2) & (l_im==2)\n\ntrade_93 &lt;- trade_93 %&gt;%\n  mutate(\n    d_border = case_when(\n      l_ex == 1 & l_im == 1 ~ 0,\n      l_ex == 2 & l_im == 2 ~ 0,\n      TRUE ~ 1\n    ),\n    lnvx = log(vx),\n    lndist = log(dist),\n    lngdp_ce = log(gdp_ce),\n    lngdp_ci = log(gdp_ci),\n    lnn_vx = lnvx - lngdp_ce - lngdp_ci\n  )\n\n# some of the FEs are dropped in Stata\n# the slopes are identical, which is what matters\n\nfit_fe &lt;- lm(\n  lnn_vx ~ lndist + d_border + as.factor(c_e) + as.factor(c_i),\n  data = trade_93\n)\n\nsummary(fit_fe)\n\n\nCall:\nlm(formula = lnn_vx ~ lndist + d_border + as.factor(c_e) + as.factor(c_i), \n    data = trade_93)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1033 -0.3934 -0.0101  0.3892  4.3379 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         8.78621    0.35661  24.638  &lt; 2e-16 ***\nlndist             -1.25168    0.03682 -33.995  &lt; 2e-16 ***\nd_border           -1.55051    0.05889 -26.327  &lt; 2e-16 ***\nas.factor(c_e)Ala  -1.63657    0.19473  -8.404  &lt; 2e-16 ***\nas.factor(c_e)Ari  -1.79305    0.19567  -9.164  &lt; 2e-16 ***\nas.factor(c_e)BC   -0.20201    0.19057  -1.060 0.289297    \nas.factor(c_e)Cal  -1.18955    0.19584  -6.074 1.59e-09 ***\nas.factor(c_e)Flo  -2.28631    0.19321 -11.833  &lt; 2e-16 ***\nas.factor(c_e)Geo  -1.64307    0.19371  -8.482  &lt; 2e-16 ***\nas.factor(c_e)Ida  -1.30647    0.19686  -6.636 4.55e-11 ***\nas.factor(c_e)Ill  -1.35755    0.19407  -6.995 4.06e-12 ***\nas.factor(c_e)Ind  -1.57817    0.19573  -8.063 1.56e-15 ***\nas.factor(c_e)Ken  -1.39710    0.19705  -7.090 2.10e-12 ***\nas.factor(c_e)Lou  -1.83182    0.19702  -9.298  &lt; 2e-16 ***\nas.factor(c_e)Mai  -1.76609    0.19648  -8.989  &lt; 2e-16 ***\nas.factor(c_e)Mas  -1.87801    0.19402  -9.679  &lt; 2e-16 ***\nas.factor(c_e)Mic  -1.58645    0.19688  -8.058 1.62e-15 ***\nas.factor(c_e)Min  -1.17706    0.19477  -6.043 1.92e-09 ***\nas.factor(c_e)MN   -0.53057    0.19094  -2.779 0.005528 ** \nas.factor(c_e)MO   -1.40491    0.19627  -7.158 1.31e-12 ***\nas.factor(c_e)Mon  -1.96394    0.19711  -9.964  &lt; 2e-16 ***\nas.factor(c_e)Mry  -1.95588    0.19703  -9.927  &lt; 2e-16 ***\nas.factor(c_e)NB   -1.09965    0.19128  -5.749 1.10e-08 ***\nas.factor(c_e)Nca  -1.55415    0.19384  -8.018 2.22e-15 ***\nas.factor(c_e)Nda  -1.85527    0.19711  -9.412  &lt; 2e-16 ***\nas.factor(c_e)Nfld -1.77882    0.19609  -9.071  &lt; 2e-16 ***\nas.factor(c_e)NHm  -1.90932    0.19548  -9.767  &lt; 2e-16 ***\nas.factor(c_e)NJr  -1.86410    0.19427  -9.595  &lt; 2e-16 ***\nas.factor(c_e)NS   -0.83361    0.19096  -4.365 1.36e-05 ***\nas.factor(c_e)Nyr  -2.46148    0.19431 -12.668  &lt; 2e-16 ***\nas.factor(c_e)Ohi  -1.42450    0.19713  -7.226 8.06e-13 ***\nas.factor(c_e)ON   -0.66004    0.19239  -3.431 0.000619 ***\nas.factor(c_e)PEI  -1.54122    0.19807  -7.781 1.37e-14 ***\nas.factor(c_e)Pen  -1.84783    0.19448  -9.501  &lt; 2e-16 ***\nas.factor(c_e)Que  -0.45829    0.19160  -2.392 0.016889 *  \nas.factor(c_e)SK   -0.69202    0.19069  -3.629 0.000295 ***\nas.factor(c_e)Ten  -1.30471    0.19528  -6.681 3.39e-11 ***\nas.factor(c_e)Tex  -1.56460    0.19550  -8.003 2.49e-15 ***\nas.factor(c_e)Ver  -2.05854    0.19683 -10.458  &lt; 2e-16 ***\nas.factor(c_e)Vir  -2.28509    0.19413 -11.771  &lt; 2e-16 ***\nas.factor(c_e)Was  -1.57505    0.19697  -7.996 2.62e-15 ***\nas.factor(c_e)Wis  -1.37702    0.19395  -7.100 1.96e-12 ***\nas.factor(c_i)Ala  -1.79804    0.19474  -9.233  &lt; 2e-16 ***\nas.factor(c_i)Ari  -1.29666    0.19730  -6.572 6.94e-11 ***\nas.factor(c_i)BC   -0.15376    0.19057  -0.807 0.419890    \nas.factor(c_i)Cal  -1.01648    0.19331  -5.258 1.68e-07 ***\nas.factor(c_i)Flo  -1.55628    0.19321  -8.055 1.66e-15 ***\nas.factor(c_i)Geo  -1.49839    0.19371  -7.735 1.94e-14 ***\nas.factor(c_i)Ida  -1.45965    0.20150  -7.244 7.09e-13 ***\nas.factor(c_i)Ill  -1.40371    0.19407  -7.233 7.68e-13 ***\nas.factor(c_i)Ind  -1.56421    0.19444  -8.045 1.80e-15 ***\nas.factor(c_i)Ken  -1.55287    0.19713  -7.877 6.58e-15 ***\nas.factor(c_i)Lou  -1.53952    0.19714  -7.809 1.11e-14 ***\nas.factor(c_i)Mai  -1.31609    0.19785  -6.652 4.11e-11 ***\nas.factor(c_i)Mas  -1.57232    0.19662  -7.997 2.61e-15 ***\nas.factor(c_i)Mic  -1.44190    0.19428  -7.422 1.98e-13 ***\nas.factor(c_i)Min  -1.44964    0.19353  -7.490 1.20e-13 ***\nas.factor(c_i)MN   -0.21433    0.19094  -1.123 0.261830    \nas.factor(c_i)MO   -1.68801    0.19369  -8.715  &lt; 2e-16 ***\nas.factor(c_i)Mon  -1.17030    0.20007  -5.849 6.11e-09 ***\nas.factor(c_i)Mry  -2.01908    0.19434 -10.389  &lt; 2e-16 ***\nas.factor(c_i)NB    0.07453    0.19128   0.390 0.696846    \nas.factor(c_i)Nca  -1.77539    0.19384  -9.159  &lt; 2e-16 ***\nas.factor(c_i)Nda  -1.32751    0.20530  -6.466 1.37e-10 ***\nas.factor(c_i)Nfld  0.21260    0.19185   1.108 0.267961    \nas.factor(c_i)NHm  -1.66954    0.19550  -8.540  &lt; 2e-16 ***\nas.factor(c_i)NJr  -1.75002    0.19427  -9.008  &lt; 2e-16 ***\nas.factor(c_i)NS   -0.68673    0.19096  -3.596 0.000334 ***\nas.factor(c_i)Nyr  -2.03481    0.19697 -10.330  &lt; 2e-16 ***\nas.factor(c_i)Ohi  -1.57975    0.19582  -8.067 1.51e-15 ***\nas.factor(c_i)ON   -0.50306    0.19239  -2.615 0.009024 ** \nas.factor(c_i)PEI  -0.42490    0.19218  -2.211 0.027200 *  \nas.factor(c_i)Pen  -1.53740    0.19580  -7.852 7.98e-15 ***\nas.factor(c_i)Que  -0.32607    0.19160  -1.702 0.089003 .  \nas.factor(c_i)SK   -0.28279    0.19069  -1.483 0.138315    \nas.factor(c_i)Ten  -1.57721    0.19401  -8.130 9.23e-16 ***\nas.factor(c_i)Tex  -1.03685    0.19296  -5.373 9.01e-08 ***\nas.factor(c_i)Ver  -1.68015    0.20291  -8.280 2.79e-16 ***\nas.factor(c_i)Vir  -2.01230    0.19542 -10.297  &lt; 2e-16 ***\nas.factor(c_i)Was  -0.86859    0.19566  -4.439 9.72e-06 ***\nas.factor(c_i)Wis  -1.41883    0.19395  -7.315 4.26e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.841 on 1430 degrees of freedom\nMultiple R-squared:  0.6639,    Adjusted R-squared:  0.6451 \nF-statistic: 35.32 on 80 and 1430 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "chapter7.html#data-description-for-feenstra-1989",
    "href": "chapter7.html#data-description-for-feenstra-1989",
    "title": "Chapter 7. Import Tariffs and Dumping",
    "section": "Data Description for Feenstra (1989)",
    "text": "Data Description for Feenstra (1989)\nThere are five data sets in excel format: cars.csv, trucks.csv, cycon.csv, cypool.csv, cyship.csv. All of the variables in the data sets are fitted values from instrumental variables regression.\n\n\n\nVariable\nDescription\n\n\n\n\niprice\nImport price\n\n\nusprice\nUS price\n\n\ngprice\nGerman price\n\n\ntariff\nTariff rate\n\n\nincome\nExpenditure on product class\n\n\nlag0\nFirst order polynomial lag on betas\n\n\nlag1\nSecond order polynomial lag on betas\n\n\nlag2\nThird order polynomial lag on betas\n\n\ny\nImport price transformed, y = iprice – income\n\n\nx1\nUS price transformed, usprice – income\n\n\nx2\nGerman price transformed, gprice – income\n\n\nz0\nFirst order polynomial lag transformed\n\n\nz1\nSecond order polynomial lag transformed\n\n\nz2\nThird order polynomial lag transformed\n\n\n\nNote: all the transformations are done to reflect their restrictions. So some are restricted to homogeneity, where others are restricted to symmetry and homogeneity.\n\nExplanation of lag0, lag1 and lag2\nWith a second-order polynomial, \\(\\alpha_i = a + bi + ci^2\\) it follows that\n\\[\\begin{align}\n\\sum_{i=0}^4 \\log(c_t^* s_{t-i}) \\alpha_i &= \\sum_{i=0}^4 \\log(c_t^* s_{t-i}) (a + bi + ci^2) \\\\\n&= a \\sum_{i=0}^4 \\log(c_t^* s_{t-i}) + b \\sum_{i=0}^4 \\log(c_t^* s_{t-i}) i + c \\sum_{i=0}^4 \\log(c_t^* s_{t-i}) i^2.\n\\end{align}\\]\nLetting \\(\\log(c_t^* s_{t-i}) = x_i\\), we can define the three lags appeating in this formula as\n\\[\\begin{align}\n\\text{lag}0 = x_0 + x_1 + x_2 + x_3 + x_4 \\\\\n\\text{lag}1 = 0 + x_1 + 2x_2 + 3x_3 + 4x_4 \\\\\n\\text{lag}2 = 0 + x_1 + 4x_2 + 9x_3 + 16x_4.\n\\end{align}\\]\nThen to compute the total pass-through of the exchange rate, it follow that,\n\\[\\begin{align}\n\\sum_{i=0}^4 \\alpha_i &= \\sum_{i=0}^4 (a + bi + ci^2) \\\\\n&= 5a + b(1 + 2 + 3 + 4) + c(1^2 + 2^2 + 3^2 + 4^2) \\\\\n&= 5a + 10b + 30c.\n\\end{align}\\]\nWhen estimating the equation using lag0, lag1, and lag2, the coefficient estimates that you obtain are a, b, and c, respectively. Using this, you can recover the coefficient estimate and standard error for each individual exchange rate term reported in the Table 7.2. You can always do this by hand, but STATA does offer a command to calculate the linear combination of the estimated coefficients. The syntax for this is,\nlincom lag0 + lag1 + lag2\nThis will calculate the coefficient estimates for the \\(\\log(c_t^* s_{t-1})\\). This is much in a same way as the syntax for test. Lag0 in above command does not refer to the data, but the coefficient estimate associated with lag0."
  },
  {
    "objectID": "chapter7.html#empirical-exercise",
    "href": "chapter7.html#empirical-exercise",
    "title": "Chapter 7. Import Tariffs and Dumping",
    "section": "Empirical exercise",
    "text": "Empirical exercise\nIn this exercise, you are asked to reproduce some of the empirical results from Feenstra (1989).\nTo complete the exercise, the files “cars.csv, trucks.csv, cycon.csv, cyship.csv, cypool.csv” should be stored in the directory: c:\\Empirical_Exercise\\Chapter_7\\. Each of these can be used in STATA programs “cars.do, trucks.do, cycon.do, cyship.do, cypool.do” to create a dataset with the variables described in “Documentation_Chp7.doc.”"
  },
  {
    "objectID": "chapter7.html#exercise-1",
    "href": "chapter7.html#exercise-1",
    "title": "Chapter 7. Import Tariffs and Dumping",
    "section": "Exercise 1",
    "text": "Exercise 1\nReplicate Table 7.2, i.e., run the specifications of (7.34) without imposing the tests of symmetry or homogeneity. Duplicate all of the coefficients that are reported in this table, except the Durbin-Watson statistics.\n\nFeenstra’s code\nCars:\nclear\ncapture log close\n\nlog using c:\\Empirical_Exercise\\Chapter_7\\cars.log, replace\n\ninsheet using c:\\Empirical_Exercise\\Chapter_7\\cars.csv\n* drop if time&lt;=12\nregress iprice time timesq lag0 lag1 lag2 usprice gprice income\n\n*i=0\nlincom lag0\n\n*i=1\nlincom lag0+lag1+lag2\n\n*i=2\nlincom lag0+2*lag1+4*lag2\n\n*i=3\nlincom lag0+3*lag1+9*lag2\n\n*i=4\nlincom lag0+4*lag1+16*lag2\n\n*summation of betai's\nlincom 5*lag0+10*lag1+30*lag2\n\n*Impose the homogeneity constraint\nregress y time timesq z0 z1 z2 x1 x2\n\n*summation of betai's\nlincom 5*z0+10*z1+30*z2\n\nlog close\nexit\nCycon:\nclear\ncapture log close\n\nlog using c:\\Empirical_Exercise\\Chapter_7\\cycon.log,replace\n\ninsheet using c:\\Empirical_Exercise\\Chapter_7\\cycon.csv\ndrop if time&lt;=16\ndrop if time&gt;=45\nregress iprice dummy1 dummy2 dummy3 time timesq lag0 lag1 lag2 /*\n     */ tariff usprice gprice income\n\n*i=0\nlincom lag0\n\n*i=1\nlincom lag0+lag1+lag2\n\n*i=2\nlincom lag0+2*lag1+4*lag2\n\n*i=3\nlincom lag0+3*lag1+9*lag2\n\n*i=4\nlincom lag0+4*lag1+16*lag2\n\n*summation of betai's\nlincom 5*lag0+10*lag1+30*lag2\n\n*Impose the homogeneity and symmetry constraints\nregress y dummy1 dummy2 dummy3 time timesq z0 z1 z2 x1 x2\n\n*summation of betai's\nlincom 5*z0+10*z1+30*z2\n\nlog close\nexit\nCypool:\nclear\ncapture log close\nlog using c:\\Empirical_Exercise\\Chapter_7\\cypool.log,replace\n\ninsheet using c:\\Empirical_Exercise\\Chapter_7\\cypool.csv\n\nregress iprice dummy1 dummy2 dummy3 time timesq lag0 lag1 lag2 /*\n        */ tariff usprice gprice income\n\n*i=0\nlincom lag0\n\n*i=1\nlincom lag0+lag1+lag2\n\n*i=2\nlincom lag0+2*lag1+4*lag2\n\n*i=3\nlincom lag0+3*lag1+9*lag2\n\n*i=4\nlincom lag0+4*lag1+16*lag2\n\n*summation of betai's\nlincom 5*lag0+10*lag1+30*lag2\n\n*Impose the homogeneity and symmetry constraints\nregress y dummy1 dummy2 dummy3 time timesq z0 z1 z2 x1 x2\n\n*summation of betai's\nlincom 5*z0+10*z1+30*z2\n\nlog close\nexit\nCyship:\nclear\ncapture log close\n\nlog using c:\\Empirical_Exercise\\Chapter_7\\cyship.log,replace\n\ninsheet using c:\\Empirical_Exercise\\Chapter_7\\cyship.csv\ndrop if time&lt;=16\nregress iprice dummy1 dummy2 dummy3 time timesq lag0 lag1 lag2 /*\n     */ tariff usprice gprice income\n\n*i=0\nlincom lag0\n\n*i=1\nlincom lag0+lag1+lag2\n\n*i=2\nlincom lag0+2*lag1+4*lag2\n\n*i=3\nlincom lag0+3*lag1+9*lag2\n\n*i=4\nlincom lag0+4*lag1+16*lag2\n\n*summation of betai's\nlincom 5*lag0+10*lag1+30*lag2\n\n*Impose the homogeneity and symmetry constraints\nregress y dummy1 dummy2 dummy3 time timesq z0 z1 z2 x1 x2\n\n*summation of betai's\nlincom 5*z0+10*z1+30*z2\n\nlog close\nexit\nTrucks:\nclear\ncapture log close\n\nlog using c:\\Empirical_Exercise\\Chapter_7\\trucks.log, replace\n\ninsheet using c:\\Empirical_Exercise\\Chapter_7\\trucks.csv\ndrop if time&lt;=12\nregress iprice time timesq lag0 lag1 lag2 tariff usprice income\n\n*i=0\nlincom lag0\n\n*i=1\nlincom lag0+lag1+lag2\n\n*i=2\nlincom lag0+2*lag1+4*lag2\n\n*i=3\nlincom lag0+3*lag1+9*lag2\n\n*i=4\nlincom lag0+4*lag1+16*lag2\n\n*summation of betai's\nlincom 5*lag0+10*lag1+30*lag2\n\n*Impose the homogeneity and symmetry constraints\nregress y time timesq z0 z1 z2 x1\n\n*summation of betai's\nlincom 5*z0+10*z1+30*z2\n\nlog close\nexit\n\n\nMy code\n\n# Packages ----\n\nlibrary(archive)\nlibrary(readr)\nlibrary(dplyr)\nlibrary(broom)\n\n# Extract ----\n\nfzip &lt;- \"first-edition/Chapter-7.zip\"\ndout &lt;- gsub(\"\\\\.zip$\", \"\", fzip)\n\nif (!dir.exists(dout)) {\n  archive_extract(fzip, dir = dout)\n}\n\n# Read and transform ----\n\nfout &lt;- paste0(dout, \"/feenstra_93.rds\")\n\nif (!file.exists(fout)) {\n  feenstra_93 &lt;- list(\n    cars = read_csv(paste0(dout, \"/cars.csv\")),\n    cycon = read_csv(paste0(dout, \"/cycon.csv\")) %&gt;%\n      filter(time &gt; 16 & time &lt; 45),\n    cypool = read_csv(paste0(dout, \"/cypool.csv\")),\n    cyship = read_csv(paste0(dout, \"/cyship.csv\")) %&gt;%\n      filter(time &gt; 16),\n    trucks = read_csv(paste0(dout, \"/trucks.csv\")) %&gt;%\n      filter(time &gt; 12)\n  )\n\n  saveRDS(feenstra_93, fout)\n} else {\n  feenstra_93 &lt;- readRDS(fout)\n}\n\n# Models ----\n\n## Cars ----\n\nmod1 &lt;- lm(\n  iprice ~ time + timesq + lag0 + lag1 + lag2 + usprice + gprice + income,\n  data = feenstra_93$cars\n)\n\nmod1_tidy &lt;- tidy(mod1)\n\n# i = 0\n\nmod1_tidy %&gt;%\n  filter(term == \"lag0\")\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic  p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 lag0     0.444     0.101      4.39 0.000286\n\n# i = 1\n# use the delta method to calculate the standard errors\n# define auxiliary variables and functions\n\nmod1_vcov &lt;- vcov(mod1)\n\nmod1_summ &lt;- summary(mod1)\n\nw &lt;- c(1, 1, 1)\n\nstd_error &lt;- function(w, X, vars) {\n  as.numeric(sqrt(w %*% X[vars, vars] %*% w))\n}\n\np_value &lt;- function(statistic, summary) {\n  as.numeric(2 * pt(-abs(statistic), df = summary$df[2]))\n}\n\nmod1_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(w = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * w),\n    std.error = std_error(w, mod1_vcov, c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, mod1_summ)\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic     p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1    0.316    0.0417      7.59 0.000000261\n\n# i = 2\n\nw &lt;- c(1, 2, 4)\n\nmod1_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 2, 4)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, mod1_vcov, c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, mod1_summ)\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1    0.166    0.0783      2.12  0.0470\n\n# i = 3\n\nw &lt;- c(1, 3, 9)\n\nmod1_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 3, 9)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, mod1_vcov, c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, mod1_summ)\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 -0.00794    0.0516    -0.154   0.879\n\n# i = 4\n\nw &lt;- c(1, 4, 16)\n\nmod1_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 4, 16)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, mod1_vcov, c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, mod1_summ)\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1   -0.205    0.0993     -2.06  0.0523\n\n# summation of betai's\n\nw &lt;- c(5, 10, 30)\n\nmod1_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, mod1_vcov, c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, mod1_summ)\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic     p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1    0.713    0.0974      7.32 0.000000450\n\n## Cycon ----\n\nmod2 &lt;- lm(\n  iprice ~ dummy1 + dummy2 + dummy3 + time + timesq + lag0 + lag1 + lag2 +\n    tariff + usprice + gprice + income,\n  data = feenstra_93$cycon\n)\n\nmod2_tidy &lt;- tidy(mod2)\n\n# i = 0\n\nmod2_tidy %&gt;%\n  filter(term == \"lag0\")\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 lag0     0.288     0.255      1.13   0.276\n\n# i = 1\n\nw &lt;- c(1, 1, 1)\n\nmod2_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(w = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * w),\n    std.error = std_error(w, vcov(mod2), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod2))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1    0.172    0.0947      1.82  0.0894\n\n# i = 2\n\nw &lt;- c(1, 2, 4)\n\nmod2_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 2, 4)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod2), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod2))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1    0.117     0.145     0.810   0.431\n\n# i = 3\n\nw &lt;- c(1, 3, 9)\n\nmod2_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 3, 9)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod2), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod2))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1    0.124    0.0837      1.48   0.159\n\n# i = 4\n\nw &lt;- c(1, 4, 16)\n\nmod2_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 4, 16)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod2), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod2))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1    0.192     0.230     0.833   0.418\n\n# summation of betai's\n\nw &lt;- c(5, 10, 30)\n\nmod2_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod2), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod2))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1    0.893     0.359      2.49  0.0252\n\n## Cypool ----\n\nmod3 &lt;- lm(\n  iprice ~ dummy1 + dummy2 + dummy3 + time + timesq + lag0 + lag1 + lag2 +\n    tariff + usprice + gprice + income,\n  data = feenstra_93$cypool\n)\n\nmod3_tidy &lt;- tidy(mod3)\n\n# i = 0\n\nmod3_tidy %&gt;%\n  filter(term == \"lag0\")\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 lag0     0.447     0.209      2.14  0.0373\n\n# i = 1\n\nw &lt;- c(1, 1, 1)\n\nmod3_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(w = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * w),\n    std.error = std_error(w, vcov(mod3), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod3))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1    0.104    0.0912      1.14   0.261\n\n# i = 2\n\nw &lt;- c(1, 2, 4)\n\nmod3_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 2, 4)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod3), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod3))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1  -0.0313     0.153    -0.204   0.839\n\n# i = 3\n\nw &lt;- c(1, 3, 9)\n\nmod3_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 3, 9)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod3), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod3))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1   0.0422    0.0852     0.496   0.622\n\n# i = 4\n\nw &lt;- c(1, 4, 16)\n\nmod3_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 4, 16)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod3), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod3))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1    0.324     0.221      1.47   0.148\n\n# summation of betai's\n\nw &lt;- c(5, 10, 30)\n\nmod3_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod3), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod3))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic  p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.886     0.213      4.16 0.000119\n\n## Cyship ----\n\nmod4 &lt;- lm(\n  iprice ~ dummy1 + dummy2 + dummy3 + time + timesq + lag0 + lag1 + lag2 +\n    tariff + usprice + gprice + income,\n  data = feenstra_93$cyship\n)\n\nmod4_tidy &lt;- tidy(mod4)\n\n# i = 0\n\nmod4_tidy %&gt;%\n  filter(term == \"lag0\")\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 lag0     0.798     0.665      1.20   0.242\n\n# i = 1\n\nw &lt;- c(1, 1, 1)\n\nmod4_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(w = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * w),\n    std.error = std_error(w, vcov(mod4), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod4))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1  -0.0417     0.260    -0.160   0.874\n\n# i = 2\n\nw &lt;- c(1, 2, 4)\n\nmod4_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 2, 4)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod4), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod4))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1   -0.335     0.523    -0.640   0.528\n\n# i = 3\n\nw &lt;- c(1, 3, 9)\n\nmod4_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 3, 9)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod4), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod4))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1  -0.0829     0.216    -0.384   0.704\n\n# i = 4\n\nw &lt;- c(1, 4, 16)\n\nmod4_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 4, 16)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod4), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod4))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1    0.715     0.758     0.943   0.355\n\n# summation of betai's\n\nw &lt;- c(5, 10, 30)\n\nmod4_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod4), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod4))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1     1.05     0.522      2.02  0.0549\n\n## Trucks ----\n\nmod5 &lt;- lm(\n  iprice ~ time + timesq + lag0 + lag1 + lag2 + tariff + usprice + income,\n  data = feenstra_93$trucks\n)\n\nmod5_tidy &lt;- tidy(mod5)\n\n# i = 0\n\nmod5_tidy %&gt;%\n  filter(term == \"lag0\")\n\n# A tibble: 1 × 5\n  term  estimate std.error statistic   p.value\n  &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 lag0     0.282    0.0563      5.00 0.0000197\n\n# i = 1\n\nw &lt;- c(1, 1, 1)\n\nmod5_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(w = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * w),\n    std.error = std_error(w, vcov(mod5), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod5))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic   p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1    0.139    0.0300      4.63 0.0000586\n\n# i = 2\n\nw &lt;- c(1, 2, 4)\n\nmod5_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 2, 4)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod5), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod5))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1   0.0608    0.0495      1.23   0.229\n\n# i = 3\n\nw &lt;- c(1, 3, 9)\n\nmod5_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 3, 9)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod5), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod5))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1   0.0472    0.0279      1.69   0.101\n\n# i = 4\n\nw &lt;- c(1, 4, 16)\n\nmod5_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = c(1, 4, 16)) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod5), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod5))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1   0.0981    0.0788      1.25   0.222\n\n# summation of betai's\n\nw &lt;- c(5, 10, 30)\n\nmod5_tidy %&gt;%\n  filter(term %in% c(\"lag0\", \"lag1\", \"lag2\")) %&gt;%\n  mutate(weight = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod5), c(\"lag0\", \"lag1\", \"lag2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod5))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic       p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1    0.627    0.0806      7.78 0.00000000713"
  },
  {
    "objectID": "chapter7.html#exercise-2",
    "href": "chapter7.html#exercise-2",
    "title": "Chapter 7. Import Tariffs and Dumping",
    "section": "Exercise 2",
    "text": "Exercise 2\nThen replicate Feenstra’s Table 2 by imposing the tests of homogeneity and symmetry, shown in (7.35a) and (7.35b). Instead of conducting the Wald test, as done in Feenstra (1989), instead conduct the analogous F-test. Do you accept or reject the hypotheses of symmetry and homogeneity?\n\nFeenstra’s code\nIncluded in exercise 1.\n\n\nMy code\n\n## Cars ----\n\n# impose the homogeneity and symmetry constraints\n\nmod1h &lt;- lm(\n  y ~ time + timesq + z0 + z1 + z2 + x1 + x2,\n  data = feenstra_93$cars\n)\n\nmod1h_tidy &lt;- tidy(mod1h)\n\n# summation of betai's\n\nw &lt;- c(5, 10, 30)\n\nmod1h_tidy %&gt;%\n  filter(term %in% c(\"z0\", \"z1\", \"z2\")) %&gt;%\n  mutate(weight = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod1h), c(\"z0\", \"z1\", \"z2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod1h))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic     p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1    0.725    0.0956      7.58 0.000000194\n\n## Cycon ----\n\n# impose the homogeneity and symmetry constraints\n\nmod2h &lt;- lm(\n  y ~ dummy1 + dummy2 + dummy3 + time + timesq + z0 + z1 + z2 + x1 + x2,\n  data = feenstra_93$cycon\n)\n\nmod2h_tidy &lt;- tidy(mod2h)\n\n# summation of betai's\n\nw &lt;- c(5, 10, 30)\n\nmod2h_tidy %&gt;%\n  filter(term %in% c(\"z0\", \"z1\", \"z2\")) %&gt;%\n  mutate(weight = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod2h), c(\"z0\", \"z1\", \"z2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod2h))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic    p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1    0.971     0.145      6.72 0.00000362\n\n## Cypool ----\n\n# impose the homogeneity and symmetry constraints\n\nmod3h &lt;- lm(\n  y ~ dummy1 + dummy2 + dummy3 + time + timesq + z0 + z1 + z2 + x1 + x2,\n  data = feenstra_93$cypool\n)\n\nmod3h_tidy &lt;- tidy(mod3h)\n\n# summation of betai's\n\nw &lt;- c(5, 10, 30)\n\nmod3h_tidy %&gt;%\n  filter(term %in% c(\"z0\", \"z1\", \"z2\")) %&gt;%\n  mutate(weight = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod3h), c(\"z0\", \"z1\", \"z2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod3h))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic       p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1     1.08     0.153      7.03 0.00000000362\n\n## Cyship ----\n\n# impose the homogeneity and symmetry constraints\n\nmod4h &lt;- lm(\n  y ~ dummy1 + dummy2 + dummy3 + time + timesq + z0 + z1 + z2 + x1 + x2,\n  data = feenstra_93$cyship\n)\n\nmod4h_tidy &lt;- tidy(mod4h)\n\n# summation of betai's\n\nw &lt;- c(5, 10, 30)\n\nmod4h_tidy %&gt;%\n  filter(term %in% c(\"z0\", \"z1\", \"z2\")) %&gt;%\n  mutate(weight = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod4h), c(\"z0\", \"z1\", \"z2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod4h))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic   p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1     1.27     0.270      4.70 0.0000734\n\n## Trucks ----\n\n# impose the homogeneity and symmetry constraints\n\nmod5h &lt;- lm(\n  y ~ time + timesq + z0 + z1 + z2 + x1,\n  data = feenstra_93$trucks\n)\n\nmod5h_tidy &lt;- tidy(mod5h)\n\n# summation of betai's\n\nw &lt;- c(5, 10, 30)\n\nmod5h_tidy %&gt;%\n  filter(term %in% c(\"z0\", \"z1\", \"z2\")) %&gt;%\n  mutate(weight = w) %&gt;%\n  summarise(\n    estimate = sum(estimate * weight),\n    std.error = std_error(weight, vcov(mod5h), c(\"z0\", \"z1\", \"z2\")),\n    statistic = estimate / std.error,\n    p.value = p_value(statistic, summary(mod5h))\n  )\n\n# A tibble: 1 × 4\n  estimate std.error statistic  p.value\n     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.582    0.0619      9.42 5.33e-11"
  }
]